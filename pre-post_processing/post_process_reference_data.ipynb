{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post processing reference data collected using CEO\n",
    "\n",
    "Collect Earth Online is being used as a tool for collecting cropland reference data, some of the points are labelled as 'mixed' or 'unsure', and others are GFSAD validation points so we need to seperate these from the dataset. We also need to seperate a validation dataset for use in the final map accuracy assessment.\n",
    "\n",
    "Inputs will be:\n",
    "\n",
    "1. `ceo-data....csv` : The results from collecting training data in the CEO tool\n",
    "\n",
    "Output will be:\n",
    "1. Validation crop/non-crop samples as a shapefile\n",
    "2. Training crop/non-crop samples as a shapefile\n",
    "3. 'Mixed' samples as a shapefile (for use in manually copllecting more training data)\n",
    "4. Crop/non-crop samples to send to RE for use in validating our data collection\n",
    "\n",
    "***\n",
    "\n",
    "*Filename guide:*\n",
    "\n",
    "* `<aez>_training_data.geojson`: The final training dataset that includes CEO data, manually collected polygons, and any pre-existing datasets.\n",
    "* `validation_samples.shp`: Validaiton data, collected using CEO, isolated from training data and only to be used for final map validation\n",
    "* `mixed_samples_points(polys).shp`: The samples labelled as 'mixed' during the CEO process, these are used for guiding manual polygon drawing to increase the amount of training data e.g. using ArcGIS\n",
    "* `ceo_td_polys.geojson` : training data polygons retrievd from CEO tool, these are combined the manually collected polygons and any pre-existing datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'data/training_validation/collect_earth/indian_ocean/'\n",
    "# csv = 'data/training_validation/collect_earth/indian_ocean/ceo-Cropland-Reference-Data-Acquisition---Indian-Ocean-Region---Ken-sample-data-2020-12-09.csv'\n",
    "csv= 'data/training_validation/collect_earth/indian_ocean/ceo-GFSAD-divergent-samples-sample-data-2020-12-10.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ground truth shapefile\n",
    "df = pd.read_csv(csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>smpl_sampleid</th>\n",
       "      <th>smpl_gfsad_clas</th>\n",
       "      <th>smpl_ken_class</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>46.883703</td>\n",
       "      <td>-23.151202</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>non-crop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46.254703</td>\n",
       "      <td>-22.720549</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>unsure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>46.153373</td>\n",
       "      <td>-22.528130</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>mixed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45.489339</td>\n",
       "      <td>-25.379383</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>non-crop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>47.384963</td>\n",
       "      <td>-23.183002</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>non-crop</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         lon        lat  smpl_sampleid  smpl_gfsad_clas  smpl_ken_class  \\\n",
       "0  46.883703 -23.151202              1                1               0   \n",
       "1  46.254703 -22.720549              2                1               0   \n",
       "2  46.153373 -22.528130              3                1               0   \n",
       "3  45.489339 -25.379383              4                1               0   \n",
       "4  47.384963 -23.183002              5                1               0   \n",
       "\n",
       "      Class  \n",
       "0  non-crop  \n",
       "1    unsure  \n",
       "2     mixed  \n",
       "3  non-crop  \n",
       "4  non-crop  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This line if entire dataset:\n",
    "df = df[['lon', 'lat', 'smpl_sampleid', 'smpl_gfsad_samp','smpl_class','Is the sample area entirely: crop, non-crop, mixed, or unsure?']]\n",
    "\n",
    "# df= df[['lon', 'lat', 'smpl_sampleid', 'smpl_gfsad_clas','smpl_ken_class','Is the sample area entirely: crop, non-crop, mixed, or unsure?']]\n",
    "\n",
    "#rename columns\n",
    "df = df.rename(columns={'Is the sample area entirely: crop, non-crop, mixed, or unsure?':'Class'})\n",
    "\n",
    "#remove nan rows\n",
    "df = df.dropna()\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove GFSAD validation points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['smpl_gfsad_samp']==False]\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seperate 'Mixed' & 'Unsure' samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples labelled as mixed: 29\n",
      "Number of samples labelled as unsure: 16\n",
      "Number of samples labelled as crop: 14\n",
      "Number of samples labelled as non-crop: 64\n"
     ]
    }
   ],
   "source": [
    "#seperate class\n",
    "mixed = df[df['Class']=='mixed']\n",
    "unsure = df[df['Class']=='unsure']\n",
    "crop = df[df['Class']=='crop']\n",
    "noncrop = df[df['Class']=='non-crop']\n",
    "\n",
    "print('Number of samples labelled as mixed: '+str(len(mixed)))\n",
    "print('Number of samples labelled as unsure: ' + str(len(unsure)))\n",
    "print('Number of samples labelled as crop: ' + str(len(crop)))\n",
    "print('Number of samples labelled as non-crop: ' + str(len(noncrop)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export mixed samples as shapefile\n",
    "mixed = gpd.GeoDataFrame(\n",
    "    mixed, geometry=gpd.points_from_xy(mixed.lon, mixed.lat, crs='EPSG:4326'))\n",
    "\n",
    "radius = 20\n",
    "\n",
    "#convert to equal area to set polygon size in metres\n",
    "mixed_poly = mixed.to_crs('EPSG:6933')\n",
    "\n",
    "#create circle buffer around points, then find envelope\n",
    "mixed_poly['geometry'] = mixed_poly['geometry'].buffer(radius).envelope\n",
    "\n",
    "#Convert back to lat/lon\n",
    "mixed_poly = mixed_poly.to_crs('EPSG:4326')\n",
    "\n",
    "#shuffle so labels aren't sequential\n",
    "mixed_poly = mixed_poly.reset_index(drop=True).drop(columns=['lon', 'lat', 'smpl_gfsad_samp'])\n",
    "\n",
    "#add plotid for CEO\n",
    "mixed_poly['PLOTID'] = range(1,len(mixed_poly)+1)\n",
    "mixed_poly['SAMPLEID'] = range(1,len(mixed_poly)+1)\n",
    "\n",
    "mixed_poly.to_file(folder + 'mixed_samples_poly.shp')\n",
    "mixed.to_file(folder + 'mixed_samples_points.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample to send to Radiant Earth\n",
    "\n",
    "Randomly select 50 crop and 50 non-crop samples to send to Radiant Earth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample 50 points from each class\n",
    "re_crop = crop.sample(n=50, replace=False).reset_index(drop=True)\n",
    "re_noncrop = noncrop.sample(n=50, replace=False).reset_index(drop=True)\n",
    "\n",
    "#merge dfs\n",
    "re_sample = pd.concat([re_crop,re_noncrop]).reset_index(drop=True).drop(columns=['smpl_gfsad_samp'])\n",
    "\n",
    "re_sample = gpd.GeoDataFrame(\n",
    "    re_sample, geometry=gpd.points_from_xy(re_sample.lon, re_sample.lat, crs='EPSG:4326')).drop(columns=['lon', 'lat'])\n",
    "\n",
    "#set radius (in metres) around points\n",
    "radius = 20\n",
    "\n",
    "#convert to equal area to set polygon size in metres\n",
    "re_sample_poly = re_sample.to_crs('EPSG:6933')\n",
    "\n",
    "#create circle buffer around points, then find envelope\n",
    "re_sample_poly['geometry'] = re_sample_poly['geometry'].buffer(radius).envelope\n",
    "\n",
    "#Convert back to lat/lon\n",
    "re_sample_poly = re_sample_poly.to_crs('EPSG:4326')\n",
    "\n",
    "#shuffle so labels aren't sequential\n",
    "re_sample_poly = re_sample_poly.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "#export to geojson\n",
    "re_sample_poly.to_file(folder+'Eastern_region_RE_sample.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isolate samples as a validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create ramdom samples\n",
    "validation_crop = crop.sample(n=100)\n",
    "validation_noncrop = noncrop.sample(n=200)\n",
    "\n",
    "#remove samples from crop/non-crop datasets\n",
    "crop = crop.drop(validation_crop.index)\n",
    "noncrop = noncrop.drop(validation_noncrop.index)\n",
    "\n",
    "#join as a single dataset\n",
    "validation_df = pd.concat([validation_crop, validation_noncrop]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "#create shapefile\n",
    "validation_df = gpd.GeoDataFrame(\n",
    "    validation_df, geometry=gpd.points_from_xy(validation_df.lon, validation_df.lat, crs='EPSG:4326'))\n",
    "\n",
    "#export to file\n",
    "validation_df.to_file(folder + 'validation_samples.shp')\n",
    "\n",
    "#plot\n",
    "validation_df.plot(column='Class', figsize=(6,6), cmap='viridis', legend=True)\n",
    "plt.title('Validation Points');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process remaining points as training data\n",
    "\n",
    "If collecting more data manually using the 'mixed' samples, then join that shapefile with this dataset after running the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td = pd.concat([crop,noncrop]).reset_index(drop=True).drop(columns=['smpl_gfsad_samp'])\n",
    "\n",
    "td = gpd.GeoDataFrame(\n",
    "    td, geometry=gpd.points_from_xy(td.LON, td.LAT, crs='EPSG:4326')).drop(columns=['LON', 'LAT'])\n",
    "\n",
    "#set radius (in metres) around points\n",
    "radius = 20\n",
    "\n",
    "#convert to equal area to set polygon size in metres\n",
    "td_poly = td.to_crs('EPSG:6933')\n",
    "\n",
    "#create circle buffer around points, then find envelope\n",
    "td_poly['geometry'] = td_poly['geometry'].buffer(radius).envelope\n",
    "\n",
    "#Convert back to lat/lon\n",
    "td_poly = td_poly.to_crs('EPSG:4326')\n",
    "\n",
    "#shuffle so labels aren't sequential\n",
    "td_poly = td_poly.sample(frac=1).reset_index(drop=True).drop(columns=['smpl_sampleid'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td_poly['Class'] = np.where(td_poly['Class']=='non-crop', 0, td_poly['Class'])\n",
    "td_poly['Class'] = np.where(td_poly['Class']=='crop', 1, td_poly['Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td_poly.to_file(folder + 'ceo_td_polys.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td.plot(column='Class', figsize=(6,6), cmap='viridis', legend=True)\n",
    "plt.title('Training Data Points');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Join training data with other data\n",
    "\n",
    "e.g. manually collected data and/or pre-existing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'data/training_validation/collect_earth/eastern/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add manually collected polygons\n",
    "manual = gpd.read_file(folder+'eastern_manual_crop_polys.shp')#.drop(['SMPL_SAMPL'],axis=1)\n",
    "manual['Class'] = np.where(manual['Class']=='crop', 1, manual['Class'])\n",
    "manual['Class'] = np.where(manual['Class']=='non-crop', 0, manual['Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample the radiant earth dataset\n",
    "re = gpd.read_file(folder+'ref_eastafrican_crops.geojson')\n",
    "\n",
    "num_of_samples=len(re)\n",
    "\n",
    "re = re.sample(n=num_of_samples).drop(['Estimated Harvest Date', 'id'], axis=1)\n",
    "re['Class'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open Collect Earth td polys\n",
    "td_poly=gpd.read_file(folder + 'Eastern_ceo_training_samples.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.concat([manual,re,td_poly]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data['Class'] = training_data['Class'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crop samples = 1951\n",
      "Non-Crop samples = 2013\n"
     ]
    }
   ],
   "source": [
    "print('Crop samples = '+str(len(training_data[training_data['Class']==1])))\n",
    "print('Non-Crop samples = '+str(len(training_data[training_data['Class']==0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.to_file(folder+'Eastern_training_data_20201215.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use this to find difference between analyst's lables and the original GFSAD stratification label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Class'] = np.where(df['Class']=='non-crop', 0, df['Class'])\n",
    "# df['Class'] = np.where(df['Class']=='crop', 1, df['Class'])\n",
    "\n",
    "# df['smpl_class'] = np.where(df['smpl_class']==1, 0, df['smpl_class'])\n",
    "# df['smpl_class'] = np.where(df['smpl_class']==2, 1, df['smpl_class'])\n",
    "\n",
    "# df = df.drop(df[df['Class']=='mixed'].index)\n",
    "# df = df.drop(df[df['Class']=='unsure'].index)\n",
    "\n",
    "# df=df[df['smpl_class']!= df['Class']]\n",
    "\n",
    "# df=df.rename(columns={'smpl_class':'gfsad_class', 'Class': 'ken_class', 'smpl_sampleid':'original_sampleid'}).drop(columns=['smpl_gfsad_samp'])\n",
    "\n",
    "# crop=df[df['gfsad_class']==1].sample(n=100, replace=False).reset_index(drop=True)\n",
    "# noncrop = df[df['gfsad_class']==0].reset_index(drop=True) #grab all points\n",
    "\n",
    "# #merge dfs\n",
    "# df = pd.concat([crop,noncrop]).reset_index(drop=True)\n",
    "\n",
    "# gdf = gpd.GeoDataFrame(\n",
    "#     df, geometry=gpd.points_from_xy(df.lon, df.lat, crs='EPSG:4326'))\n",
    "\n",
    "# radius = 20\n",
    "\n",
    "# #convert to equal area to set polygon size in metres\n",
    "# gdf_poly = gdf.to_crs('EPSG:6933')\n",
    "\n",
    "# #create circle buffer around points, then find envelope\n",
    "# gdf_poly['geometry'] = gdf_poly['geometry'].buffer(radius).envelope\n",
    "\n",
    "# #Convert back to lat/lon\n",
    "# gdf_poly = gdf_poly.to_crs('EPSG:4326')\n",
    "\n",
    "# #shuffle so labels aren't sequential\n",
    "# gdf_poly = gdf_poly.reset_index(drop=True).drop(columns=['lon', 'lat'])\n",
    "\n",
    "# #add plotid for CEO\n",
    "# gdf_poly['PLOTID'] = range(1,len(gdf_poly)+1)\n",
    "# gdf_poly['SAMPLEID'] = range(1,len(gdf_poly)+1)\n",
    "\n",
    "# gdf_poly.to_file(folder + 'gfsad_divergence_samples_poly.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
