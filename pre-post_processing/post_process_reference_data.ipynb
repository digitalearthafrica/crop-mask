{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post processing reference data collected using CEO\n",
    "\n",
    "Collect Earth Online is being used as a tool for collecting cropland reference data, some of the points are labelled as 'mixed' or 'unsure', and others are GFSAD validation points so we need to seperate these from the dataset. We also need to seperate a validation dataset for use in the final map accuracy assessment.\n",
    "\n",
    "Inputs will be:\n",
    "\n",
    "1. `ceo-data....csv` : The results from collecting training data in the CEO tool\n",
    "\n",
    "Output will be:\n",
    "1. Validation crop/non-crop samples as a shapefile\n",
    "2. Training crop/non-crop samples as a shapefile\n",
    "3. 'Mixed' samples as a shapefile (for use in manually copllecting more training data)\n",
    "4. Crop/non-crop samples to send to RE for use in validating our data collection\n",
    "\n",
    "***\n",
    "\n",
    "*Filename guide:*\n",
    "\n",
    "* `<aez>_training_data.geojson`: The final training dataset that includes CEO data, manually collected polygons, and any pre-existing datasets.\n",
    "* `validation_samples.shp`: Validation data, collected using CEO, isolated from training data and only to be used for final map validation\n",
    "* `mixed_samples_points(polys).shp`: The samples labelled as 'mixed' during the CEO process, these are used for guiding manual polygon drawing to increase the amount of training data e.g. using ArcGIS\n",
    "* `ceo_td_polys.geojson` : training data polygons retrievd from CEO tool, these are combined the manually collected polygons and any pre-existing datasets to produce the `<aez>_training_data.geojson` file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'data/training_validation/collect_earth/central/'\n",
    "csv = folder+'ceo-Cropland-Reference-Data-Acquisition---Central---Victor-sample-data-2020-12-16.csv'\n",
    "csv_1 = folder+'ceo-Cropland-Reference-Data-Acquisition---Southern-Region--Julius-sample-data-2021-02-08.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ground truth shapefile\n",
    "df = pd.read_csv(csv)\n",
    "df1 = pd.read_csv(csv_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This line if entire dataset:\n",
    "df = df[['lon', 'lat', 'smpl_sampleid', 'smpl_gfsad_samp','smpl_class','Is the sample area entirely: crop, non-crop, mixed, or unsure?']]\n",
    "df1 = df1[['lon', 'lat', 'smpl_sampleid', 'smpl_gfsad_samp','smpl_class','Is the sample area entirely: crop, non-crop, mixed, or unsure?']]\n",
    "\n",
    "#rename columns\n",
    "df = df.rename(columns={'Is the sample area entirely: crop, non-crop, mixed, or unsure?':'Class'})\n",
    "df1 = df1.rename(columns={'Is the sample area entirely: crop, non-crop, mixed, or unsure?':'Class'})\n",
    "\n",
    "#remove nan rows\n",
    "# df = df.dropna()\n",
    "print(len(df))\n",
    "print(len(df1))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df,df1]).drop(columns=['smpl_class'])\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove GFSAD validation points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['smpl_gfsad_samp']==False]\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seperate 'Mixed' & 'Unsure' samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seperate class\n",
    "mixed = df[df['Class']=='mixed'].reset_index(drop=True)\n",
    "unsure = df[df['Class']=='unsure'].reset_index(drop=True)\n",
    "crop = df[df['Class']=='crop'].reset_index(drop=True)\n",
    "noncrop = df[df['Class']=='non-crop'].reset_index(drop=True)\n",
    "\n",
    "print('Number of samples labelled as mixed: '+str(len(mixed)))\n",
    "print('Number of samples labelled as unsure: ' + str(len(unsure)))\n",
    "print('Number of samples labelled as crop: ' + str(len(crop)))\n",
    "print('Number of samples labelled as non-crop: ' + str(len(noncrop)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export mixed samples as shapefile\n",
    "mixed = gpd.GeoDataFrame(\n",
    "    mixed, geometry=gpd.points_from_xy(mixed.lon, mixed.lat, crs='EPSG:4326'))\n",
    "\n",
    "radius = 20\n",
    "\n",
    "#convert to equal area to set polygon size in metres\n",
    "mixed_poly = mixed.to_crs('EPSG:6933')\n",
    "\n",
    "#create circle buffer around points, then find envelope\n",
    "mixed_poly['geometry'] = mixed_poly['geometry'].buffer(radius).envelope\n",
    "\n",
    "#Convert back to lat/lon\n",
    "mixed_poly = mixed_poly.to_crs('EPSG:4326')\n",
    "\n",
    "#shuffle so labels aren't sequential\n",
    "mixed_poly = mixed_poly.reset_index(drop=True).drop(columns=['lon', 'lat']) #'smpl_gfsad_samp'\n",
    "\n",
    "#add plotid for CEO\n",
    "mixed_poly['PLOTID'] = range(1,len(mixed_poly)+1)\n",
    "mixed_poly['SAMPLEID'] = range(1,len(mixed_poly)+1)\n",
    "\n",
    "mixed_poly.to_file(folder + 'mixed_samples_poly.shp')\n",
    "mixed.to_file(folder + 'mixed_samples_points.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample to send to Radiant Earth\n",
    "\n",
    "Randomly select crop and non-crop samples to send to Radiant Earth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name='Central_region_RE_sample.geojson'\n",
    "re_crop_sample_size=50\n",
    "re_non_crop_sample_size=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample 50 points from each class\n",
    "re_crop = crop.sample(n=re_crop_sample_size, replace=False).reset_index(drop=True)\n",
    "re_noncrop = noncrop.sample(n=re_non_crop_sample_size, replace=False).reset_index(drop=True)\n",
    "\n",
    "#merge dfs\n",
    "re_sample = pd.concat([re_crop,re_noncrop]).reset_index(drop=True).drop(columns=['smpl_gfsad_samp'])\n",
    "\n",
    "re_sample = gpd.GeoDataFrame(\n",
    "    re_sample, geometry=gpd.points_from_xy(re_sample.lon, re_sample.lat, crs='EPSG:4326')).drop(columns=['lon', 'lat'])\n",
    "\n",
    "#set radius (in metres) around points\n",
    "radius = 20\n",
    "\n",
    "#convert to equal area to set polygon size in metres\n",
    "re_sample_poly = re_sample.to_crs('EPSG:6933')\n",
    "\n",
    "#create circle buffer around points, then find envelope\n",
    "re_sample_poly['geometry'] = re_sample_poly['geometry'].buffer(radius).envelope\n",
    "\n",
    "#Convert back to lat/lon\n",
    "re_sample_poly = re_sample_poly.to_crs('EPSG:4326')\n",
    "\n",
    "#shuffle so labels aren't sequential\n",
    "re_sample_poly = re_sample_poly.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "#export to geojson\n",
    "re_sample_poly.to_file(folder+name, driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isolate samples as a validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_sample_size=50\n",
    "non_crop_sample_size=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create ramdom samples\n",
    "validation_crop = crop.sample(n=crop_sample_size)\n",
    "validation_noncrop = noncrop.sample(n=non_crop_sample_size)\n",
    "\n",
    "#remove samples from crop/non-crop datasets\n",
    "crop = crop.drop(validation_crop.index)\n",
    "noncrop = noncrop.drop(validation_noncrop.index)\n",
    "\n",
    "#join as a single dataset\n",
    "validation_df = pd.concat([validation_crop, validation_noncrop]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "#create shapefile\n",
    "validation_df = gpd.GeoDataFrame(\n",
    "    validation_df, geometry=gpd.points_from_xy(validation_df.lon, validation_df.lat, crs='EPSG:4326'))\n",
    "\n",
    "#export to file\n",
    "validation_df.to_file(folder + 'validation_samples.shp')\n",
    "\n",
    "#plot\n",
    "validation_df.plot(column='Class', figsize=(6,6), cmap='viridis', legend=True)\n",
    "plt.title('Validation Points');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process remaining points as training data\n",
    "\n",
    "If collecting more data manually using the 'mixed' samples, then join that shapefile with this dataset after running the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td = pd.concat([crop,noncrop]).reset_index(drop=True).drop(columns=['smpl_gfsad_samp'])\n",
    "\n",
    "td = gpd.GeoDataFrame(\n",
    "    td, geometry=gpd.points_from_xy(td.lon, td.lat, crs='EPSG:4326')).drop(columns=['lon', 'lat'])\n",
    "\n",
    "#set radius (in metres) around points\n",
    "radius = 20\n",
    "\n",
    "#convert to equal area to set polygon size in metres\n",
    "td_poly = td.to_crs('EPSG:6933')\n",
    "\n",
    "#create circle buffer around points, then find envelope\n",
    "td_poly['geometry'] = td_poly['geometry'].buffer(radius).envelope\n",
    "\n",
    "#Convert back to lat/lon\n",
    "td_poly = td_poly.to_crs('EPSG:4326')\n",
    "\n",
    "#shuffle so labels aren't sequential\n",
    "td_poly = td_poly.sample(frac=1).reset_index(drop=True).drop(columns=['smpl_sampleid'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td_poly['Class'] = np.where(td_poly['Class']=='non-crop', 0, td_poly['Class'])\n",
    "td_poly['Class'] = np.where(td_poly['Class']=='crop', 1, td_poly['Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td_poly.to_file(folder + 'ceo_td_polys.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td.plot(column='Class', figsize=(6,6), cmap='viridis', legend=True)\n",
    "plt.title('Training Data Points');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('crop = '+str(len(td_poly[td_poly.Class==1])))\n",
    "print('noncrop = '+str(len(td_poly[td_poly.Class==0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Join training data with other data\n",
    "\n",
    "e.g. manually collected data and/or pre-existing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'data/training_validation/collect_earth/eastern/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add manually collected polygons\n",
    "manual = gpd.read_file(folder+'eastern_manual_crop_polys.shp')#.drop(['SMPL_SAMPL'],axis=1)\n",
    "manual['Class'] = np.where(manual['Class']=='crop', 1, manual['Class'])\n",
    "manual['Class'] = np.where(manual['Class']=='non-crop', 0, manual['Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample the radiant earth dataset\n",
    "re = gpd.read_file(folder+'ref_eastafrican_crops.geojson')\n",
    "\n",
    "num_of_samples=len(re)\n",
    "\n",
    "re = re.sample(n=num_of_samples).drop(['Estimated Harvest Date', 'id'], axis=1)\n",
    "re['Class'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open Collect Earth td polys\n",
    "td_poly=gpd.read_file(folder + 'ceo_td_polys.geojson')\n",
    "# td_poly=gpd.read_file(folder + 'Eastern_ceo_training_samples.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.concat([manual,re,td_poly]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data['Class'] = training_data['Class'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('No. of samples: '+str(len(training_data)))\n",
    "print('Crop samples = '+str(len(training_data[training_data['Class']==1])))\n",
    "print('Non-Crop samples = '+str(len(training_data[training_data['Class']==0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.to_file(folder+'Eastern_training_data_20210301.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use this to find difference between analyst's lables and the original GFSAD stratification label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Class'] = np.where(df['Class']=='non-crop', 0, df['Class'])\n",
    "# df['Class'] = np.where(df['Class']=='crop', 1, df['Class'])\n",
    "\n",
    "# df['smpl_class'] = np.where(df['smpl_class']==1, 0, df['smpl_class'])\n",
    "# df['smpl_class'] = np.where(df['smpl_class']==2, 1, df['smpl_class'])\n",
    "\n",
    "# df = df.drop(df[df['Class']=='mixed'].index)\n",
    "# df = df.drop(df[df['Class']=='unsure'].index)\n",
    "\n",
    "# df=df[df['smpl_class']!= df['Class']]\n",
    "\n",
    "# df=df.rename(columns={'smpl_class':'gfsad_class', 'Class': 'ken_class', 'smpl_sampleid':'original_sampleid'}).drop(columns=['smpl_gfsad_samp'])\n",
    "\n",
    "# crop=df[df['gfsad_class']==1].sample(n=100, replace=False).reset_index(drop=True)\n",
    "# noncrop = df[df['gfsad_class']==0].reset_index(drop=True) #grab all points\n",
    "\n",
    "# #merge dfs\n",
    "# df = pd.concat([crop,noncrop]).reset_index(drop=True)\n",
    "\n",
    "# gdf = gpd.GeoDataFrame(\n",
    "#     df, geometry=gpd.points_from_xy(df.lon, df.lat, crs='EPSG:4326'))\n",
    "\n",
    "# radius = 20\n",
    "\n",
    "# #convert to equal area to set polygon size in metres\n",
    "# gdf_poly = gdf.to_crs('EPSG:6933')\n",
    "\n",
    "# #create circle buffer around points, then find envelope\n",
    "# gdf_poly['geometry'] = gdf_poly['geometry'].buffer(radius).envelope\n",
    "\n",
    "# #Convert back to lat/lon\n",
    "# gdf_poly = gdf_poly.to_crs('EPSG:4326')\n",
    "\n",
    "# #shuffle so labels aren't sequential\n",
    "# gdf_poly = gdf_poly.reset_index(drop=True).drop(columns=['lon', 'lat'])\n",
    "\n",
    "# #add plotid for CEO\n",
    "# gdf_poly['PLOTID'] = range(1,len(gdf_poly)+1)\n",
    "# gdf_poly['SAMPLEID'] = range(1,len(gdf_poly)+1)\n",
    "\n",
    "# gdf_poly.to_file(folder + 'gfsad_divergence_samples_poly.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
