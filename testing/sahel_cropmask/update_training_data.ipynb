{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update training data with manually drawn polygons\n",
    "\n",
    "This notebook will merge manually drawn crop/non-crop polygons (done either QGIS or ArcGIS) with the training data collected using Collect Earth.\n",
    "\n",
    "During each iteration of this procedure, update the suffix of the output file with the date of creation in format YYYYMMDD, this will help keep track of which iteration of training data is used for which set of classifications.\n",
    "\n",
    "***\n",
    "\n",
    "*Filename guide:*\n",
    "\n",
    "* `<aez>_training_data_<YYYYMMDD>.geojson`: The training dataset that includes CEO data, manually collected polygons, and any pre-existing datasets.\n",
    "* `ceo_td_polys.geojson` : training data polygons retrievd from Collect Earth, these are combined the manually collected polygons and any pre-existing datasets to produce the `<aez>_training_data_<date_of_creation>.geojson` file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_suffix='20211029'\n",
    "\n",
    "ceo_td_path = 'data/ceo_td_polys.geojson' #shouldn't need to change this\n",
    "\n",
    "manual_poly_path = 'data/sahel_manual_crop_polys.shp' #the file you've been adding new TD polygons too in GIS.\n",
    "\n",
    "extras_path = 'data/senegal_crop.geojson' #data provide by IPs (cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open vector files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add manually collected polygons\n",
    "manual = gpd.read_file(manual_poly_path)\n",
    "ceo = gpd.read_file(ceo_td_path)\n",
    "extras = gpd.read_file(extras_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reclassify Class field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual['Class'] = np.where(manual['Class']=='crop', 1, manual['Class'])\n",
    "manual['Class'] = np.where(manual['Class']=='non-crop', 0, manual['Class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge files together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.concat([manual,ceo, extras]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensure class is in integer type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data['Class'] = training_data['Class'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counts for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of samples: 5075\n",
      "Crop samples = 1814\n",
      "Non-Crop samples = 3261\n"
     ]
    }
   ],
   "source": [
    "print('No. of samples: '+str(len(training_data)))\n",
    "print('Crop samples = '+str(len(training_data[training_data['Class']==1])))\n",
    "print('Non-Crop samples = '+str(len(training_data[training_data['Class']==0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data=training_data.drop('smpl_class', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to disk\n",
    "\n",
    "This file will be the new training data to pass into the `1_Extract_training_data.ipynb` notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.to_file('data/sahel_training_data_'+date_suffix+'.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Cleaning IP provided training data from Senegal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_path = 'data/Parcelles Kédougou 21 Décembre.shp'\n",
    "b_path = 'data/Parcelles Kolda 21 Décembre.shp'\n",
    "c_path = 'data/Parcelles Tambacounda 21 Décembre.shp'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=gpd.read_file(a_path)\n",
    "b=gpd.read_file(b_path)\n",
    "c=gpd.read_file(c_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### combine all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_all = pd.concat([a,b,c]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate area in hectares and filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_all = _all.to_crs('epsg:6933')\n",
    "_all['area_ha'] = _all['geometry'].area / 10000\n",
    "_all = _all[_all['area_ha']<=1.0] #less than 1 hectare\n",
    "_all = _all[_all['area_ha']>=0.1] #greater than 0.1 hectare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample crop-specific datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 25\n",
    "\n",
    "rice_low = _all[_all['CodeSpé']=='1'].sample(n=n).reset_index(drop=True)\n",
    "rice_high = _all[_all['CodeSpé']=='2'].sample(n=n).reset_index(drop=True)\n",
    "millet = _all[_all['CodeSpé']=='3'].sample(n=n).reset_index(drop=True)\n",
    "maize = _all[_all['CodeSpé']=='4'].sample(n=50).reset_index(drop=True) #more for maize\n",
    "peas = _all[_all['CodeSpé']=='6'].sample(n=n).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### concat and clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = pd.concat([rice_low,rice_high, millet, maize, peas]).reset_index(drop=True)\n",
    "cols = list(out.columns)\n",
    "cols.remove('geometry')\n",
    "out['Class'] = 1\n",
    "out = out.drop(cols, axis=1)\n",
    "out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.to_crs('epsg:4326').to_file('data/senegal_crop.shp')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
