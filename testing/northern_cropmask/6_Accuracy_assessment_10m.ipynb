{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validating the 10m Northern Africa Cropland Mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "Previously, in the `6_Accuracy_assessment_20m.ipynb` notebook, we were doing preliminary validations on 20m resolution testing crop-masks. The crop-mask was stored on disk as a geotiff. The final cropland extent mask, produced at 10m resolution, is stored in the datacube and requires a different method for validating.\n",
    "\n",
    "> NOTE: A very big sandbox is required (256GiB RAM) to run this script. \n",
    "\n",
    "This notebook will output a `confusion error matrix` containing Overall, Producer's, and User's accuracy, along with the F1 score for each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Getting started\n",
    "\n",
    "To run this analysis, run all the cells in the notebook, starting with the \"Load packages\" cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import rasterio\n",
    "import datacube\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "from sklearn.metrics import f1_score\n",
    "from rasterstats import zonal_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Parameters\n",
    "\n",
    "* `product` : name of crop-mask we're validating\n",
    "* `bands`: the bands of the crop-mask we want to load and validate. Can one of either `'mask'` or `'filtered'`\n",
    "* `grd_truth` : a shapefile containing crop/no-crop points to serve as the \"ground-truth\" dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product = \"crop_mask_northern\"\n",
    "band = 'filtered'\n",
    "grd_truth = 'data/validation_samples.shp'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Load the datasets\n",
    "\n",
    "`the cropland extent mask`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#connect to the datacube\n",
    "dc = datacube.Datacube(app='feature_layers')\n",
    "    \n",
    "#load 10m cropmask\n",
    "ds = dc.load(product=product, measurements=[band]).squeeze()\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Ground truth points`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ground truth shapefile\n",
    "ground_truth = gpd.read_file(grd_truth).to_crs('EPSG:6933')\n",
    "\n",
    "# rename the class column to 'actual'\n",
    "ground_truth = ground_truth.rename(columns={'Class':'Actual'})\n",
    "\n",
    "# reclassifer into int\n",
    "ground_truth['Actual'] = np.where(ground_truth['Actual']=='non-crop', 0, ground_truth['Actual'])\n",
    "ground_truth['Actual'] = np.where(ground_truth['Actual']=='crop', 1, ground_truth['Actual'])\n",
    "ground_truth.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Convert points into polygons\n",
    "\n",
    "When the validation data was collected, 40x40m polygons were evaluated as either crop/non-crop rather than points, so we want to sample the raster using the same small polygons. We'll find the majority or 'mode' statistic within the polygon and use that to compare with the validation dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set radius (in metres) around points\n",
    "radius = 20\n",
    "\n",
    "#create circle buffer around points, then find envelope\n",
    "ground_truth['geometry'] = ground_truth['geometry'].buffer(radius).envelope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate zonal statistics\n",
    "\n",
    "We want to know what the majority pixel value is inside each validation polygon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_majority(x):\n",
    "    a=np.ma.MaskedArray.count(x)\n",
    "    b=np.sum(x)\n",
    "    c=b/a\n",
    "    if c>0.5:\n",
    "        return 1\n",
    "    if c<=0.5:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate stats\n",
    "stats = zonal_stats(ground_truth.geometry,\n",
    "                    ds[band].values,\n",
    "                    affine=ds.geobox.affine,\n",
    "                    add_stats={'majority':custom_majority},\n",
    "                    nodata=255)\n",
    "\n",
    "#append stats to grd truth df\n",
    "ground_truth['Prediction']=[i['majority'] for i in stats]\n",
    "\n",
    "ground_truth.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Create a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = pd.crosstab(ground_truth['Actual'],\n",
    "                               ground_truth['Prediction'],\n",
    "                               rownames=['Actual'],\n",
    "                               colnames=['Prediction'],\n",
    "                               margins=True)\n",
    "\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate User's and Producer's Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Producer's Accuracy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix[\"Producer's\"] = [confusion_matrix.loc[0, 0] / confusion_matrix.loc[0, 'All'] * 100,\n",
    "                              confusion_matrix.loc[1, 1] / confusion_matrix.loc[1, 'All'] * 100,\n",
    "                              np.nan]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`User's Accuracy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_accuracy = pd.Series([confusion_matrix[0][0] / confusion_matrix[0]['All'] * 100,\n",
    "                                confusion_matrix[1][1] / confusion_matrix[1]['All'] * 100]\n",
    "                         ).rename(\"User's\")\n",
    "\n",
    "confusion_matrix = confusion_matrix.append(users_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Overall Accuracy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix.loc[\"User's\",\"Producer's\"] = (confusion_matrix.loc[0, 0] + \n",
    "                                                confusion_matrix.loc[1, 1]) / confusion_matrix.loc['All', 'All'] * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`F1 Score`\n",
    "\n",
    "The F1 score is the harmonic mean of the precision and recall, where an F1 score reaches its best value at 1 (perfect precision and recall), and is calculated as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{Fscore} = 2 \\times \\frac{\\text{UA} \\times \\text{PA}}{\\text{UA} + \\text{PA}}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Where UA = Users Accuracy, and PA = Producer's Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fscore = pd.Series([(2*(confusion_matrix.loc[\"User's\", 0]*confusion_matrix.loc[0, \"Producer's\"]) / (confusion_matrix.loc[\"User's\", 0]+confusion_matrix.loc[0, \"Producer's\"])) / 100,\n",
    "                    f1_score(ground_truth['Actual'].astype(np.int8), ground_truth['Prediction'].astype(np.int8), average='binary')]\n",
    "                         ).rename(\"F-score\")\n",
    "\n",
    "confusion_matrix = confusion_matrix.append(fscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tidy Confusion Matrix\n",
    "\n",
    "* Limit decimal places,\n",
    "* Add readable class names\n",
    "* Remove non-sensical values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# round numbers\n",
    "confusion_matrix = confusion_matrix.round(decimals=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename booleans to class names\n",
    "confusion_matrix = confusion_matrix.rename(columns={0:'Non-crop', 1:'Crop', 'All':'Total'},\n",
    "                                            index={0:'Non-crop', 1:'Crop', 'All':'Total'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the nonsensical values in the table\n",
    "confusion_matrix.loc[\"User's\", 'Total'] = '--'\n",
    "confusion_matrix.loc['Total', \"Producer's\"] = '--'\n",
    "confusion_matrix.loc[\"F-score\", 'Total'] = '--'\n",
    "confusion_matrix.loc[\"F-score\", \"Producer's\"] = '--'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix.to_csv('results/Northern_10m_accuracy_assessment_confusion_matrix.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Additional information\n",
    "\n",
    "**License:** The code in this notebook is licensed under the [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0). \n",
    "Digital Earth Africa data is licensed under the [Creative Commons by Attribution 4.0](https://creativecommons.org/licenses/by/4.0/) license.\n",
    "\n",
    "**Contact:** If you need assistance, please post a question on the [Open Data Cube Slack channel](http://slack.opendatacube.org/) or on the [GIS Stack Exchange](https://gis.stackexchange.com/questions/ask?tags=open-data-cube) using the `open-data-cube` tag (you can view previously asked questions [here](https://gis.stackexchange.com/questions/tagged/open-data-cube)).\n",
    "If you would like to report an issue with this notebook, you can file one on [Github](https://github.com/digitalearthafrica/deafrica-sandbox-notebooks).\n",
    "\n",
    "**Last modified:** Dec 2020\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
