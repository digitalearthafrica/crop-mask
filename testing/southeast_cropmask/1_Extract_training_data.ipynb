{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting training data from the ODC\n",
    "\n",
    "* **Products used:** \n",
    "[gm_s2_semiannual](https://explorer.digitalearth.africa/gm_s2_semiannual)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "This notebook will extract training data over Southern Africa using geometries within a shapefile (or geojson). To do this, we rely on a custom `deafrica-sandbox-notebooks` function called `collect_training_data`, contained within the [deafrica_tools.classification](https://github.com/digitalearthafrica/deafrica-sandbox-notebooks/blob/minty-fresh-sandbox/Tools/deafrica_tools/classification.py) script.\n",
    "\n",
    "1. Import, and preview our training data contained in the file: `'data/Southern_training_data_YYYYMMDD.geojson'`\n",
    "2. Extract training data from the datacube using a custom defined feature layer function that we can pass to `collect_training_data`. The training data function is stored in the python file `feature_layer_functions.py` - the functions are stored in a seperate file simply to keep this notebook tidy.\n",
    "\n",
    "    - **The features used to create the cropland mask are as follows:**\n",
    "        - For two seasons, January to June, and July to Decemeber:\n",
    "            - A geomedian composite of nine Sentinel-2 spectral bands\n",
    "            - Three measures of median absolute deviation\n",
    "            - NDVI, MNDWI, and LAI\n",
    "            - Cumulative Rainfall from CHIRPS\n",
    "        - Slope from SRTM (not seasonal, obviously)\n",
    "            \n",
    "3. Separate the coordinate values in the returned training data from step 2, and export the coordinates as a text file.\n",
    "4. Export the remaining training data (features other than coordinates) to disk as a text file for use in subsequent scripts\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "To run this analysis, run all the cells in the notebook, starting with the \"Load packages\" cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import datacube\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import subprocess as sp\n",
    "import geopandas as gpd\n",
    "from odc.io.cgroups import get_cpu_quota\n",
    "from datacube.utils.geometry import assign_crs\n",
    "from datacube.utils.rio import configure_s3_access\n",
    "configure_s3_access(aws_unsigned=True, cloud_defaults=True)\n",
    "\n",
    "from deafrica_tools.plotting import map_shapefile\n",
    "from deafrica_tools.classification import collect_training_data \n",
    "\n",
    "#import the custom feature layer functions\n",
    "from feature_layer_functions import gm_mads_two_seasons_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis parameters\n",
    "\n",
    "* `path`: The path to the input shapefile from which we will extract training data.\n",
    "* `field`: This is the name of column in your shapefile attribute table that contains the class labels. **The class labels must be integers**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/Southeast_training_data_20211022.geojson'\n",
    "\n",
    "output_suffix = '20211022'\n",
    "\n",
    "field = 'Class'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatically find the number of cpus\n",
    "\n",
    "> **Note**: With supervised classification, its common to have many, many labelled geometries in the training data. `collect_training_data` can parallelize across the geometries in order to speed up the extracting of training data. Setting `ncpus>1` will automatically trigger the parallelization, however, its best to set `ncpus=1` to begin with to assist with debugging before triggering the parallelization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ncpus=round(get_cpu_quota())\n",
    "ncpus=29\n",
    "print('ncpus = '+str(ncpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & preview polygon data\n",
    "\n",
    "We can load and preview our input data shapefile using `geopandas`. The shapefile should contain a column with class labels (e.g. 'class'). These labels will be used to train our model. \n",
    "\n",
    "> Remember, the class labels **must** be represented by `integers`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load input data shapefile\n",
    "input_data = gpd.read_file(path)\n",
    "\n",
    "# Plot first five rows\n",
    "input_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training data in an interactive map\n",
    "# map_shapefile(input_data, attribute=field)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can pass this shapefile to `collect_training_data`.  For each of the geometries in our shapefile we will extract features in accordance with the function `feature_layer_functions.gm_mads_two_seasons_training`. These will include:\n",
    "\n",
    "For two seasons, January to June, and July to Decemeber:\n",
    "- A geomedian composite of nine Sentinel-2 spectral bands\n",
    "- Three measures of median absolute deviation\n",
    "- NDVI, MNDWI, and LAI\n",
    "- Cumulative Rainfall from the CHIRPS\n",
    "- Slope from SRTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to set up a few extra inputs for `collect_training_data` and the datacube.  See the function docs [here](https://github.com/digitalearthafrica/deafrica-sandbox-notebooks/blob/03b7b41d5f6526ff3f33618f7a0b48c0d10a155f/Scripts/deafrica_classificationtools.py#L650) for more information on these parameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up our inputs to collect_training_data\n",
    "zonal_stats = 'median'\n",
    "return_coords = True\n",
    "\n",
    "# Set up the inputs for the ODC query\n",
    "time = ('2019')\n",
    "measurements = [\n",
    "        \"blue\",\n",
    "        \"green\",\n",
    "        \"red\",\n",
    "        \"nir\",\n",
    "        \"swir_1\",\n",
    "        \"swir_2\",\n",
    "        \"red_edge_1\",\n",
    "        \"red_edge_2\",\n",
    "        \"red_edge_3\",\n",
    "        \"bcdev\",\n",
    "        \"edev\",\n",
    "        \"sdev\"\n",
    "]\n",
    "resolution = (-10, 10)\n",
    "output_crs = 'epsg:6933'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate a new datacube query object\n",
    "query = {\n",
    "    'time': time,\n",
    "    'measurements': measurements,\n",
    "    'resolution': resolution,\n",
    "    'output_crs': output_crs,\n",
    "    'group_by' : 'solar_day',\n",
    "    'resampling': 'bilinear'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract training data\n",
    "\n",
    "> Remember, if running this function for the first time, its advisable to set `ncpus=1` to assist with debugging before triggering the parallelization (which won't return errors if something is not working correctly).  You can also limit the number of polygons to run for the first time by passing in `gdf=input_data[0:5]`, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "column_names, model_input = collect_training_data(\n",
    "                                    gdf=input_data,\n",
    "                                    dc_query=query,\n",
    "                                    ncpus=ncpus,\n",
    "                                    return_coords=return_coords,\n",
    "                                    field=field,\n",
    "                                    zonal_stats=zonal_stats,\n",
    "                                    fail_threshold=0.01,\n",
    "                                    feature_func=gm_mads_two_seasons_training\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(column_names)\n",
    "print('')\n",
    "print(np.array_str(model_input, precision=2, suppress_small=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export training data\n",
    "\n",
    "Once we've collected all the training data we require, we can write the data to disk. This will allow us to import the data in the next step(s) of the workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the name and location of the output file\n",
    "output_file = \"results/training_data/southeast_training_data_\"+output_suffix+\".txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grab all columns except the x-y coords\n",
    "model_col_indices = [column_names.index(var_name) for var_name in column_names]\n",
    "#Export files to disk\n",
    "np.savetxt(output_file, model_input[:, model_col_indices], header=\" \".join(column_names), fmt=\"%4f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "To continue working through the notebooks in this `Southern Africa Cropland Mask` workflow, go to the next notebook `2_Inspect_training_data.ipynb`.\n",
    "\n",
    "1. **Extract_training_data (this notebook)** \n",
    "2. [Inspect_training_data](2_Inspect_training_data.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Additional information\n",
    "\n",
    "**License:** The code in this notebook is licensed under the [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0). \n",
    "Digital Earth Africa data is licensed under the [Creative Commons by Attribution 4.0](https://creativecommons.org/licenses/by/4.0/) license.\n",
    "\n",
    "**Contact:** If you need assistance, please post a question on the [Open Data Cube Slack channel](http://slack.opendatacube.org/) or on the [GIS Stack Exchange](https://gis.stackexchange.com/questions/ask?tags=open-data-cube) using the `open-data-cube` tag (you can view previously asked questions [here](https://gis.stackexchange.com/questions/tagged/open-data-cube)).\n",
    "If you would like to report an issue with this notebook, you can file one on [Github](https://github.com/digitalearthafrica/deafrica-sandbox-notebooks).\n",
    "\n",
    "**Last modified:** Dec 2020\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
