{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting training data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "Prior to training a machine learning classifier, it can be useful to understand which  of our feature layers are most useful for distinguishing between classes. The feature layers the model is trained on form the **knowledge base** of the algorithm. We can explore this knowledge base using class-specific [violin plots](https://en.wikipedia.org/wiki/Violin_plot#:~:text=A%20violin%20plot%20is%20a,by%20a%20kernel%20density%20estimator.), and through a dimensionality reduction approach called [principal-components analysis](https://builtin.com/data-science/step-step-explanation-principal-component-analysis). The latter transforms our large dataset with lots of variables into a smaller dataset with fewer variables (while still preserving much of the variance), this allows us to visualise a very complex dataset in a relatively intuitive and straightforward manner.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "Using the training data written to file in the previous notebook, [1_Extract_training_data](1_Extract_training_data.ipynb), this notebook will:\n",
    "\n",
    "1. Plot class-specific violin plots for each of the feature layers in the training data.\n",
    "2. Plot the importance of each feature after applying a model to the data.\n",
    "3. Calculate the first two and three prinicpal components of the dataset and plot them as 2D and 3D scatter plots.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "To run this analysis, run all the cells in the notebook, starting with the \"Load packages\" cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "from sklearn.decomposition import PCA\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Parameters\n",
    "\n",
    "* `training_data`: Name and location of the training data `.txt` file output from runnning `1_Extract_training_data.ipynb`\n",
    "* `class_dict`: A dictionary mapping the 'string' name of the classes to the integer values that represent our classes in the training data (e.g. `{'crop': 1., 'noncrop': 0.}`)\n",
    "* `field`: This is the name of column in the original training data shapefile that contains the class labels. This is provided simply so we can remove this attribute before we plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = \"results/training_data/southeast_training_data_20211022\"\n",
    "\n",
    "class_dict = {'crop':1, 'noncrop':0}\n",
    "\n",
    "field = 'Class'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "model_input = np.loadtxt(training_data)\n",
    "\n",
    "# load the column_names\n",
    "with open(training_data, 'r') as file:\n",
    "    header = file.readline()\n",
    "    \n",
    "column_names = header.split()[1:]\n",
    "# Extract relevant indices from training data\n",
    "model_col_indices = [column_names.index(var_name) for var_name in column_names[1:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling\n",
    "\n",
    "This cell extracts each class in the training data array and assigns it to a dictionary, this step will allow for cleanly plotting our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {}\n",
    "for key, value in class_dict.items():\n",
    "    print(key, value)\n",
    "    # extract values for class from training data\n",
    "    arr = model_input[model_input[:,0]==value]\n",
    "    # create a pandas df for ease of use later\n",
    "    df = pd.DataFrame(arr).rename(columns={i:column_names[i] for i in range(0,len(column_names))}).drop(field, axis=1)\n",
    "    # Scale the dataframe\n",
    "    scaled_df = StandardScaler(with_mean=False).fit_transform(df)\n",
    "    scaled_df = pd.DataFrame(scaled_df, columns=df.columns)\n",
    "    \n",
    "    dfs.update({key:scaled_df})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature layer violin plots\n",
    "\n",
    "The code here will generate class-specific violin plots for each feature side-by-side so we can see how seperable the features are between the classes.  Features that distinguish between crop and non-crop will have medians and distributions that do not overlap too much  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate a random list of colors same length as num of classes \n",
    "get_colors = lambda n: list(map(lambda i: \"#\" + \"%06x\" % random.randint(0, 0xFFFFFF),range(n)))\n",
    "colors = get_colors(len(dfs))\n",
    "\n",
    "#generate list of offsets & widths for plotting\n",
    "start=-0.2 \n",
    "end=0.2\n",
    "offsets = list(np.linspace(start,end,len(dfs)))\n",
    "if len(dfs) == 2:\n",
    "    width=0.4\n",
    "else:\n",
    "    width=np.abs(offsets[0] - offsets[1])\n",
    "\n",
    "#create figure and axes \n",
    "fig, ax = plt.subplots(figsize=(40,8))\n",
    "\n",
    "for key, color, offset in zip(dfs,colors, offsets):\n",
    "    #create violin plots\n",
    "    pp = ax.violinplot(dfs[key].values,\n",
    "                       showmedians=True,\n",
    "                       positions=np.arange(dfs[key].values.shape[1])+offset, widths=width\n",
    "                      )\n",
    "    # change the colour of the plots\n",
    "    for pc in pp['bodies']:\n",
    "        pc.set_facecolor(color)\n",
    "        pc.set_edgecolor(color)\n",
    "        pc.set_alpha(1)\n",
    "    \n",
    "    #change the line style in the plots\n",
    "    for partname in ('cbars','cmins','cmaxes','cmedians'):\n",
    "        vp = pp[partname]\n",
    "        vp.set_edgecolor('black')\n",
    "        vp.set_linewidth(1)\n",
    "\n",
    "#tidy the plot, add a title and legend\n",
    "ax.set_xticks(np.arange(len(column_names[1:])))\n",
    "ax.set_xticklabels(column_names[1:])\n",
    "ax.set_ylim(-5.0,12)\n",
    "ax.set_xlim(-0.5,len(column_names[1:])-.5)\n",
    "ax.set_ylabel(\"Scaled Values\", fontsize=14)\n",
    "ax.set_xlabel(\"Feature Layers\", fontsize=14)\n",
    "ax.set_title(\"Training Data Knowledge-Base\", fontsize=14)\n",
    "ax.legend([Patch(facecolor=c) for c in colors], [key for key in dfs], loc='upper right');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance\n",
    "\n",
    "Here we extract classifier estimates of the relative importance of each feature for training the classifier. This is useful for potentially selecting a subset of input bands/variables for model training/classification (i.e. optimising feature space). However, in this case, we are not selecting a subset of features, but rather just trying to understand the importance of each feature.\n",
    "\n",
    "Results will be presented in asscending order with the most important features listed last. Importance is reported as a relative fraction between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=RandomForestClassifier()\n",
    "model.fit(model_input[:, model_col_indices], model_input[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = np.argsort(model.feature_importances_)\n",
    "\n",
    "plt.figure(figsize=(35,10))\n",
    "plt.bar(x=np.array(column_names[1:])[order],\n",
    "        height=model.feature_importances_[order])\n",
    "# plt.bar(x=column_names[1:], height=model.feature_importances_)\n",
    "plt.gca().set_ylabel('Importance', labelpad=10)\n",
    "plt.gca().set_xlabel('Feature', labelpad=10)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis\n",
    "\n",
    "The code below will calculate and plot the first two and three principal components of our training dataset. \n",
    "\n",
    "The first step is to standardise our data to express each feature layer in terms of mean and standard deviation, this is necessary because principal component analysis is quite sensitive to the variances of the initial variables. If there are large differences between the ranges of initial variables, those variables with larger ranges will dominate over those with small ranges (For example, a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which will lead to biased results. So, transforming the data to comparable scales can prevent this problem.  We do this using sklearn's `StandardScalar` function which will normalise the values in an array to the array's mean and standard deviation via the formuala: `z = (x-u/s)`, where `u` is the mean of and `s` is the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mean and variance for each feature\n",
    "x = StandardScaler().fit_transform(model_input[:,1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conduct the PCAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#two components\n",
    "pca2 = PCA(n_components=2)\n",
    "pca2_fit = pca2.fit_transform(x)\n",
    "#three PCA components\n",
    "pca3 = PCA(n_components=3)\n",
    "pca3_fit = pca3.fit_transform(x)\n",
    "\n",
    "#add back to df\n",
    "pca2_df = pd.DataFrame(data = pca2_fit,\n",
    "                      columns = ['PC1', 'PC2'])\n",
    "pca3_df = pd.DataFrame(data = pca3_fit,\n",
    "                      columns = ['PC1', 'PC2', 'PC3'])\n",
    "\n",
    "# concat with classes\n",
    "result2 = pd.concat([pca2_df, pd.DataFrame({'class':model_input[:,0]})], axis=1)\n",
    "result3 = pd.concat([pca3_df, pd.DataFrame({'class':model_input[:,0]})], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2,b2 = pca2.explained_variance_ratio_\n",
    "a3,b3,c3 = pca3.explained_variance_ratio_\n",
    "print(\"Variance explained by two principal components = \" + str(round((a2+b2)*100, 2))+\" %\")\n",
    "print(\"Variance explained by three principal components = \" + str(round((a3+b3+c3)*100, 2))+\" %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot both 2 & 3 principal components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(result2) > 5000:\n",
    "    result2=result2.sample(n=5000)\n",
    "if len(result3) > 5000:\n",
    "    result3=result3.sample(n=5000)\n",
    "\n",
    "fig = plt.figure(figsize=(18,8))\n",
    "fig.suptitle('Training data: Principal Component Analysis', fontsize=14)\n",
    "\n",
    "# First subplot\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "scatter1=sns.scatterplot(x=\"PC1\", y=\"PC2\",\n",
    "           data=result2,\n",
    "           hue='class',\n",
    "           hue_norm=tuple(np.unique(result2['class'])),\n",
    "           palette='viridis',\n",
    "           legend=False,\n",
    "           alpha=0.7,                    \n",
    "           ax=ax\n",
    "          )\n",
    "\n",
    "ax.set_title('Two Principal Components', fontsize=14)\n",
    "ax.grid(True)\n",
    "\n",
    "# Second subplot\n",
    "ax = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "scatter2 = ax.scatter(result3['PC1'], result3['PC2'], result3['PC3'],\n",
    "                      c=result3['class'], s=60, alpha=0.5)\n",
    "\n",
    "# make simple, bare axis lines through space:\n",
    "xAxisLine = ((min(result3['PC1']), max(result3['PC1'])), (0, 0), (0,0))\n",
    "ax.plot(xAxisLine[0], xAxisLine[1], xAxisLine[2], 'r')\n",
    "yAxisLine = ((0, 0), (min(result3['PC2']), max(result3['PC2'])), (0,0))\n",
    "ax.plot(yAxisLine[0], yAxisLine[1], yAxisLine[2], 'r')\n",
    "zAxisLine = ((0, 0), (0,0), (min(result3['PC3']), max(result3['PC3'])))\n",
    "ax.plot(zAxisLine[0], zAxisLine[1], zAxisLine[2], 'r')\n",
    "\n",
    "ax.set_title('Three Principal Components', fontsize=14)\n",
    "ax.set_xlabel(\"PC1\")\n",
    "ax.set_ylabel(\"PC2\")\n",
    "ax.set_zlabel(\"PC3\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "To continue working through the notebooks in this `Southern Africa Cropland Mask` workflow, go to the next notebook `3_Train_fit_evaluate_classifier.ipynb`.\n",
    "\n",
    "1. [Extract_training_data](1_Extract_training_data.ipynb) \n",
    "2. **Inspect_training_data (this notebook)**\n",
    "3. [Train_fit_evaluate_classifier](3_Train_fit_evaluate_classifier.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Additional information\n",
    "\n",
    "**License:** The code in this notebook is licensed under the [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0). \n",
    "Digital Earth Africa data is licensed under the [Creative Commons by Attribution 4.0](https://creativecommons.org/licenses/by/4.0/) license.\n",
    "\n",
    "**Contact:** If you need assistance, please post a question on the [Open Data Cube Slack channel](http://slack.opendatacube.org/) or on the [GIS Stack Exchange](https://gis.stackexchange.com/questions/ask?tags=open-data-cube) using the `open-data-cube` tag (you can view previously asked questions [here](https://gis.stackexchange.com/questions/tagged/open-data-cube)).\n",
    "If you would like to report an issue with this notebook, you can file one on [Github](https://github.com/digitalearthafrica/deafrica-sandbox-notebooks).\n",
    "\n",
    "**Last modified:** Dec 2020\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
