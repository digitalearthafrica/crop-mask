{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction <img align=\"right\" src=\"../figs/DE_Africa_Logo_Stacked_RGB_small.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "Using the model we created in the `3_Train_fit_evaluate_classifier.ipynb`, this notebook will make predictions on new data to generate a cropland mask for Eastern Africa. The notebook will ceate both pixel-wise classifications and classification probabilities. Results are saved to disk as Cloud-Optimised-Geotiffs.\n",
    "\n",
    "1. Open and inspect the shapefile which delineates the extent we're classifying\n",
    "2. Import the model\n",
    "3. Make predictions on new data loaded through the ODC.  The pixel classification will also undergo a post-processing step where steep slopes and water are masked using a SRTM derivative and WOfS, respectively. Pixels labelled as crop above 3600 metres ASL are also masked. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Getting started\n",
    "\n",
    "To run this analysis, run all the cells in the notebook, starting with the \"Load packages\" cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -e ../../production/dea_ml\n",
    "# !pip install git+https://github.com/digitalearthafrica/deafrica-sandbox-notebooks.git@minty-fresh-sandbox#subdirectory=Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import datacube\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "from joblib import load\n",
    "from odc.algo import xr_reproject\n",
    "from datacube.utils import geometry\n",
    "from datacube.utils.cog import write_cog\n",
    "from datacube.testutils.io import rio_slurp_xarray\n",
    "\n",
    "from deafrica_tools.dask import create_local_dask_cluster\n",
    "from deafrica_tools.classification import predict_xr, HiddenPrints\n",
    "from deafrica_tools.plotting import map_shapefile\n",
    "from deafrica_tools.spatial import xr_rasterize\n",
    "\n",
    "from dea_ml.core.africa_geobox import AfricaGeobox\n",
    "\n",
    "#import out feature layer function for prediction\n",
    "from feature_layer_functions import gm_mads_two_seasons_prediction\n",
    "from post_processing import post_processing\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up a dask cluster\n",
    "This will help keep our memory use down and conduct the analysis in parallel. If you'd like to view the dask dashboard, click on the hyperlink that prints below the cell. You can use the dashboard to monitor the progress of calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_local_dask_cluster()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis parameters\n",
    "\n",
    "* `model_path`: The path to the location where the model exported from the previous notebook is stored\n",
    "* `training_data`: Name and location of the training data `.txt` file output from runnning `1_Extract_training_data.ipynb`\n",
    "* `test_shapefile`: A shapefile containing polygons that represent regions where you want to test your model. The shapefile should have a unique identifier as this will be used to export classification results to disk as geotiffs. Alternatively, this could be a shapefile that defines the extent of the entire AOI you want to classify.\n",
    "* `results`: A folder location to store the classified geotiffs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'results/gm_mads_two_seasons_ml_model_20210427.joblib'\n",
    "\n",
    "training_data = \"results/training_data/gm_mads_two_seasons_training_data_20210427.txt\"\n",
    "\n",
    "test_shapefile = 'data/s2_tiles_eastern_aez.geojson'\n",
    "\n",
    "results = 'results/classifications/'\n",
    "\n",
    "output_suffix = '20210427'\n",
    "\n",
    "dask_chunks = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open and inspect test_shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_file(test_shapefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdf.head()\n",
    "# map_shapefile(gdf, attribute='title')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open the model\n",
    "\n",
    "The code below will also re-open the training data we exported from `3_Train_fit_evaluate_classifier.ipynb`\n",
    "\n",
    "and wwe import the column names of the feature layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "model_input = np.loadtxt(training_data)\n",
    "\n",
    "# load the column_names\n",
    "with open(training_data, 'r') as file:\n",
    "    header = file.readline()\n",
    "    \n",
    "column_names = header.split()[1:][1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop through test tiles and predict (with post-processing)\n",
    "\n",
    "For every tile we list in the `test_shapefile`, we calculate the feature layers, and then use the DE Africa function `predict_xr` to classify the data.\n",
    "\n",
    "The `feature_layer_functions.gm_mads_two_seasons_prediction` and `post_processing.post_processing` functions are doing most of the heavy-lifting here\n",
    "\n",
    "The results are exported to file as Cloud-Optimised Geotiffs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a folder to store results in if one doesn't already exist\n",
    "if not os.path.exists(results+output_suffix):\n",
    "        os.mkdir(results+output_suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf[216:217]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#get affine of all tiles as geobox object\n",
    "geobox_dict = AfricaGeobox(resolution=20)#.geobox_dict\n",
    "\n",
    "i=1\n",
    "for index, row in gdf[216:217].iterrows():\n",
    "    \n",
    "    #grab x,y id from the geopan\n",
    "    x,y = gdf.loc[index]['title'].split(\",\")\n",
    "    x, y = int(x)+181, int(y)+77\n",
    "    \n",
    "    print('working on tile: '+str(x)+','+str(y),str(i)+\"/\"+str(len(gdf)),end='\\r')\n",
    "\n",
    "    #get affine of single tile as geobox object\n",
    "    geobox = geobox_dict[(x,y)]\n",
    "    \n",
    "    #run feature layer function\n",
    "    data = gm_mads_two_seasons_prediction(geobox, dask_chunks)\n",
    "    data = data[column_names]\n",
    "    \n",
    "   #predict using the imported model\n",
    "    with HiddenPrints():\n",
    "        predicted = predict_xr(model,\n",
    "                           data,\n",
    "                           proba=True,\n",
    "                           persist=True,\n",
    "                           clean=True,\n",
    "                           return_input=True\n",
    "                          )#.compute()\n",
    "        \n",
    "    predicted = predicted.chunk({'x':-1, 'y':-1})\n",
    "    #-------Post-processsing ------------------------------    \n",
    "    ds = post_processing(predicted)\n",
    "    ds = ds.compute()\n",
    "#     #----export classifications to disk-----------------------\n",
    "# #     write_cog(predict,\n",
    "# #               results+output_suffix+'/Eastern_'+str(x)+'_'+str(y)+'_prediction_pixel_'+output_suffix+'.tif',\n",
    "# #               overwrite=True)\n",
    "    break\n",
    "#     i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.mask.plot(size=8);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.prob.plot(size=8);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.filtered.plot(size=8);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "To continue working through the notebooks in this `Eastern Africa Cropland Mask` workflow, go to the next notebook `5_Object-based_filtering.ipynb`.\n",
    "\n",
    "1. [Extracting_training_data](1_Extracting_training_data.ipynb) \n",
    "2. [Inspect_training_data](2_Inspect_training_data.ipynb)\n",
    "3. [Train_fit_evaluate_classifier](3_Train_fit_evaluate_classifier.ipynb)\n",
    "4. **Predict (this notebook)**\n",
    "5. [Object-based_filtering](5_Object-based_filtering.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Additional information\n",
    "\n",
    "**License:** The code in this notebook is licensed under the [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0). \n",
    "Digital Earth Africa data is licensed under the [Creative Commons by Attribution 4.0](https://creativecommons.org/licenses/by/4.0/) license.\n",
    "\n",
    "**Contact:** If you need assistance, please post a question on the [Open Data Cube Slack channel](http://slack.opendatacube.org/) or on the [GIS Stack Exchange](https://gis.stackexchange.com/questions/ask?tags=open-data-cube) using the `open-data-cube` tag (you can view previously asked questions [here](https://gis.stackexchange.com/questions/tagged/open-data-cube)).\n",
    "If you would like to report an issue with this notebook, you can file one on [Github](https://github.com/digitalearthafrica/deafrica-sandbox-notebooks).\n",
    "\n",
    "**Last modified:** Dec 2020\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
