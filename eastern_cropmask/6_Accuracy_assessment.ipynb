{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy assessment of the Eastern Africa Cropland Mask<img align=\"right\" src=\"../figs/DE_Africa_Logo_Stacked_RGB_small.jpg\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "Now that we have run classifications for the Eastern Africa AEZ, its time to conduct an accuracy assessment. The data used for assessing the accuracy was collected previously and set aside. Its stored in the data/ folder: `data/Validation_samples.shp` \n",
    "\n",
    "This notebook will output a `confusion error matrix` containing Overall, Producer's, and User's accuracy, along with the F1 score for each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Getting started\n",
    "\n",
    "To run this analysis, run all the cells in the notebook, starting with the \"Load packages\" cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import rasterio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "sys.path.append('../Scripts')\n",
    "from deafrica_spatialtools import zonal_stats_parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Parameters\n",
    "\n",
    "* `pred_tif` : a binary classification of crop/no-crop output by the ML script.\n",
    "* `grd_truth` : a shapefile containing crop/no-crop points to serve as the \"ground-truth\" dataset\n",
    "* `aez_region` : a shapefile used to limit the ground truth points to the region where the model has classified crop/non-crop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_tif = \"/g/data/u23/data/crop-mask/prediction/Eastern_gm_mads_two_seasons_20210203_mosaic.tif\"\n",
    "pred_tif = \"results/classifications/predicted/20210301/Eastern_gm_mads_two_seasons_20210301_mosaic_clipped.tif\"\n",
    "# grd_truth = '../pre-post_processing/data/training_validation/GFSAD2015/cropland_prelim_validation_GFSAD.shp'\n",
    "# grd_truth = 'data/validation_samples.shp'\n",
    "aez_region = 'data/Eastern.shp'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the datasets\n",
    "\n",
    "`Ground truth points`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ground truth shapefile\n",
    "ground_truth = gpd.read_file(grd_truth).to_crs('EPSG:6933')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the class column to 'actual'\n",
    "ground_truth = ground_truth.rename(columns={'Class':'Actual'})\n",
    "ground_truth.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reclassify 'Actual' column to match the raster values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open shapefile\n",
    "aez=gpd.read_file(aez_region).to_crs('EPSG:6933')\n",
    "# clip points to region\n",
    "ground_truth = gpd.overlay(ground_truth,aez, how='intersection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth['Actual'] = np.where(ground_truth['Actual']=='non-crop', 0, ground_truth['Actual'])\n",
    "ground_truth['Actual'] = np.where(ground_truth['Actual']=='crop', 1, ground_truth['Actual'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This cell if point sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Point sampling of raster for validation purpose\n",
    "prediction = rasterio.open(pred_tif)\n",
    "coords = [(x,y) for x, y in zip(ground_truth.geometry.x, ground_truth.geometry.y)]\n",
    "# Sample the raster at every point location and store values in DataFrame\n",
    "ground_truth['Prediction'] = [int(x[0]) for x in prediction.sample(coords)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The next two cells if polygon sampling\n",
    "#### Convert points into polygons\n",
    "\n",
    "When the validation data was collected, 40x40m polygons were evaluated as either crop/non-crop rather than points, so we want to sample the raster using the same small polygons. We'll find the majority or 'mode' statistic within the polygon and use that to compare with the validation dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set radius (in metres) around points\n",
    "radius = 20\n",
    "\n",
    "#convert to equal area to set polygon size in metres\n",
    "ground_truth = ground_truth\n",
    "\n",
    "#create circle buffer around points, then find envelope\n",
    "ground_truth['geometry'] = ground_truth['geometry'].buffer(radius).envelope\n",
    "\n",
    "#export to file for use in zonal-stats\n",
    "ground_truth.to_file(grd_truth[:-4]+\"_poly.shp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate zonal statistics\n",
    "\n",
    "We want to know what the majority pixel value is inside each validation polygon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zonal_stats_parallel(shp=grd_truth[:-4]+\"_poly.shp\",\n",
    "                    raster=pred_tif,\n",
    "                    statistics=['majority'],\n",
    "                    out_shp=grd_truth[:-4]+\"_poly.shp\",\n",
    "                    ncpus=2,\n",
    "                    nodata=-999)\n",
    "\n",
    "#read in the results\n",
    "x=gpd.read_file(grd_truth[:-4]+\"_poly.shp\")\n",
    "\n",
    "#add result to original ground truth array\n",
    "ground_truth['Prediction'] = x['majority'].astype(np.int16)\n",
    "\n",
    "#Remove the temporary shapefile we made\n",
    "[os.remove(i) for i in glob.glob(grd_truth[:-4]+\"_poly\"+'*')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Create a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = pd.crosstab(ground_truth['Actual'],\n",
    "                               ground_truth['Prediction'],\n",
    "                               rownames=['Actual'],\n",
    "                               colnames=['Prediction'],\n",
    "                               margins=True)\n",
    "\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate User's and Producer's Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Producer's Accuracy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix[\"Producer's\"] = [confusion_matrix.loc[0, 0] / confusion_matrix.loc[0, 'All'] * 100,\n",
    "                              confusion_matrix.loc[1, 1] / confusion_matrix.loc[1, 'All'] * 100,\n",
    "                              np.nan]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`User's Accuracy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_accuracy = pd.Series([confusion_matrix[0][0] / confusion_matrix[0]['All'] * 100,\n",
    "                                confusion_matrix[1][1] / confusion_matrix[1]['All'] * 100]\n",
    "                         ).rename(\"User's\")\n",
    "\n",
    "confusion_matrix = confusion_matrix.append(users_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Overall Accuracy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix.loc[\"User's\",\"Producer's\"] = (confusion_matrix.loc[0, 0] + \n",
    "                                                confusion_matrix.loc[1, 1]) / confusion_matrix.loc['All', 'All'] * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`F1 Score`\n",
    "\n",
    "The F1 score is the harmonic mean of the precision and recall, where an F1 score reaches its best value at 1 (perfect precision and recall), and is calculated as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{Fscore} = 2 \\times \\frac{\\text{UA} \\times \\text{PA}}{\\text{UA} + \\text{PA}}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Where UA = Users Accuracy, and PA = Producer's Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fscore = pd.Series([(2*(confusion_matrix.loc[\"User's\", 0]*confusion_matrix.loc[0, \"Producer's\"]) / (confusion_matrix.loc[\"User's\", 0]+confusion_matrix.loc[0, \"Producer's\"])) / 100,\n",
    "                    f1_score(ground_truth['Actual'].astype(np.int8), ground_truth['Prediction'].astype(np.int8), average='binary')]\n",
    "                         ).rename(\"F-score\")\n",
    "\n",
    "confusion_matrix = confusion_matrix.append(fscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tidy Confusion Matrix\n",
    "\n",
    "* Limit decimal places,\n",
    "* Add readable class names\n",
    "* Remove non-sensical values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# round numbers\n",
    "confusion_matrix = confusion_matrix.round(decimals=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename booleans to class names\n",
    "confusion_matrix = confusion_matrix.rename(columns={0:'Non-crop', 1:'Crop', 'All':'Total'},\n",
    "                                            index={0:'Non-crop', 1:'Crop', 'All':'Total'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the nonsensical values in the table\n",
    "confusion_matrix.loc[\"User's\", 'Total'] = '--'\n",
    "confusion_matrix.loc['Total', \"Producer's\"] = '--'\n",
    "confusion_matrix.loc[\"F-score\", 'Total'] = '--'\n",
    "confusion_matrix.loc[\"F-score\", \"Producer's\"] = '--'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix.to_csv('results/Eastern_confusion_matrix.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "This is the last notebook in the `Eastern Africa Cropland Mask` workflow! To revist any of the other notebooks, use the links below.\n",
    "\n",
    "1. [Extracting_training_data](1_Extracting_training_data.ipynb) \n",
    "2. [Inspect_training_data](2_Inspect_training_data.ipynb)\n",
    "3. [Train_fit_evaluate_classifier](3_Train_fit_evaluate_classifier.ipynb)\n",
    "4. [Predict](4_Predict.ipynb)\n",
    "5. [Object-based_filtering](5_Object-based_filtering.ipynb)\n",
    "6. **Accuracy_assessment (this notebook)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Additional information\n",
    "\n",
    "**License:** The code in this notebook is licensed under the [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0). \n",
    "Digital Earth Africa data is licensed under the [Creative Commons by Attribution 4.0](https://creativecommons.org/licenses/by/4.0/) license.\n",
    "\n",
    "**Contact:** If you need assistance, please post a question on the [Open Data Cube Slack channel](http://slack.opendatacube.org/) or on the [GIS Stack Exchange](https://gis.stackexchange.com/questions/ask?tags=open-data-cube) using the `open-data-cube` tag (you can view previously asked questions [here](https://gis.stackexchange.com/questions/tagged/open-data-cube)).\n",
    "If you would like to report an issue with this notebook, you can file one on [Github](https://github.com/digitalearthafrica/deafrica-sandbox-notebooks).\n",
    "\n",
    "**Last modified:** Dec 2020\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
