<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>dea_ml.core.merge_tifs_to_ds API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>dea_ml.core.merge_tifs_to_ds</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import datetime
import json
import math
import os
import os.path as osp
import re
import uuid
from typing import List, Optional, Dict, Tuple

import click
import joblib
import numpy as np
import psutil
import xarray as xr
from datacube import Datacube
from datacube.testutils.io import rio_slurp_xarray
from datacube.utils.cog import write_cog
from datacube.utils.dask import start_local_dask
from datacube.utils.geometry import assign_crs, GeoBox, Geometry
from datacube.utils.rio import configure_s3_access
from distributed import Client
from odc.algo import xr_reproject
from odc.io.cgroups import get_cpu_quota, get_mem_quota
from odc.stats._cli_common import setup_logging
from pyproj import Proj, transform

from dea_ml.core.africa_geobox import AfricaGeobox
from dea_ml.core.cm_prediction import predict_xr
from dea_ml.core.product_feature_config import FeaturePathConfig, prepare_the_io_path
from dea_ml.core.stac_to_dc import StacIntoDc


def get_max_mem() -&gt; int:
    &#34;&#34;&#34;
    Max available memory, takes into account pod resource allocation
    &#34;&#34;&#34;
    total = psutil.virtual_memory().total
    mem_quota = get_mem_quota()
    if mem_quota is None:
        return total
    return min(mem_quota, total)


def get_max_cpu() -&gt; int:
    &#34;&#34;&#34;
    Max available CPU (rounded up if fractional), takes into account pod
    resource allocation
    &#34;&#34;&#34;
    ncpu = get_cpu_quota()
    if ncpu is not None:
        return int(math.ceil(ncpu))
    return psutil.cpu_count()


class PredictFromFeature:
    &#34;&#34;&#34;
    This only covers 2019 case in sandbox now. Check configureation in FeaturePathConfig before use
    run this.
    # todo: add context  to this classpredicted
    &#34;&#34;&#34;

    def __init__(
        self,
        config: Optional[FeaturePathConfig] = None,
        geobox_dict: Optional[Dict] = None,
        client: Optional[Client] = None,
        gm_ds: Optional[xr.Dataset] = None,
    ):
        self.config = config if config else FeaturePathConfig()
        self.geobox_dict = geobox_dict
        if not client:
            nthreads = get_max_cpu()
            memory_limit = get_max_mem()
            client = start_local_dask(
                threads_per_worker=nthreads,
                processes=False,
                memory_limit=int(0.9 * memory_limit),
            )
            configure_s3_access(aws_unsigned=True, cloud_defaults=True, client=client)
        self.client = client
        self.gm_ds = gm_ds

    def merge_ds_exec(self, x: int, y: int) -&gt; Tuple[str, xr.Dataset]:
        &#34;&#34;&#34;
        merge the xarray dataset
        @param x: tile index x
        #param y: time inde y
        @return: subfolder path and the xarray dataset of the features
        &#34;&#34;&#34;
        subfld = &#34;x{x:+04d}/y{y:+04d}&#34;.format(x=x, y=y)
        P6M_tifs: Dict = get_tifs_paths(self.config.TIF_path, subfld)
        geobox = self.geobox_dict[(x, y)]
        seasoned_ds = {}
        for k, tifs in P6M_tifs.items():
            era = &#34;_S1&#34; if &#34;2019-01--P6M&#34; in k else &#34;_S2&#34;
            if not self.gm_ds:
                # no prepare base ds
                base_ds = merge_tifs_into_ds(
                    k, tifs, rename_dict=self.config.rename_dict
                )
            else:
                base_ds = self.gm_ds
            # TODO: to validate the 6month geomedia is down scaled already.
            base_ds = down_scale_gm_band(base_ds)

            seasoned_ds[era] = complete_gm_mads(base_ds, geobox, era)

        slope = (
            rio_slurp_xarray(self.config.url_slope, gbox=geobox)
            .drop(&#34;spatial_ref&#34;)
            .to_dataset(name=&#34;slope&#34;)
        )
        return (
            subfld,
            xr.merge(
                [seasoned_ds[&#34;_S1&#34;], seasoned_ds[&#34;_S2&#34;], slope], compat=&#34;override&#34;
            ).chunk({&#34;x&#34;: -1, &#34;y&#34;: -1}),
        )

    def run(self, taskstr: str):
        &#34;&#34;&#34;
        run the prediction here, default crs=&#39;epsg:4326&#39;
        The sample of a feature:
        @return: None
        &#34;&#34;&#34;
        setup_logging()
        import logging  # noqa pylint: disable=import-outside-toplevel

        _log = logging.getLogger(__name__)
        if not self.geobox_dict:
            self.geobox_dict = AfricaGeobox(
                resolution=self.config.resolution,
                crs=self.config.output_crs,
            ).geobox_dict

        x, y = get_xy_from_task(taskstr)

        # step 1: collect the geomedian, indices, rainfall, slope as feature
        # TODO: add more interface in merge_ds_exec, dc.load and x, y
        # adapt to other africa_geobox type
        subfld, data = self.merge_ds_exec(x, y)
        input_data = data[self.config.training_features]

        # step 2: load trained model
        model = joblib.load(self.config.model_path)

        # step 3: prediction
        predicted = predict_xr(
            model,
            input_data,
            clean=True,
            proba=True,
        )

        _log.info(&#34;... Dask computing ...&#34;)
        # predicted.compute()
        predicted.persist()

        # post prediction filtering
        predict = predicted.Predictions
        query = self.config.query.copy()
        # Update dc query with geometry
        geobox_used = self.geobox_dict[(x, y)]
        query[&#34;geopolygon&#34;] = Geometry(geobox_used.extent.geom, crs=geobox_used.crs)

        dc = Datacube(app=__name__)
        # mask with WOFS
        # wofs_query = query.pop(&#34;measurements&#34;)
        wofs = dc.load(product=&#34;ga_ls8c_wofs_2_summary&#34;, **query)
        wofs = wofs.frequency &gt; 0.2  # threshold
        predict = predict.where(~wofs, 0)

        # mask steep slopes
        slope = data.slope &gt; 35
        predict = predict.where(~slope, 0)

        # mask where the elevation is above 3600m
        query.pop(&#34;time&#34;)
        elevation = dc.load(product=&#34;srtm&#34;, **query)
        elevation = elevation.elevation &gt; 3600
        predict = predict.where(~elevation.squeeze(), 0)
        predict = predict.astype(np.uint8)

        output_fld, paths, metadata_path = prepare_the_io_path(self.config, subfld)

        if not osp.exists(output_fld):
            os.makedirs(output_fld)

        _log.info(&#34;collecting mask and write cog.&#34;)
        write_cog(
            predict.compute(),
            paths[&#34;mask&#34;],
            overwrite=True,
        )

        _log.info(&#34;collecting prob and write cog.&#34;)
        write_cog(
            predicted.Probabilities.astype(np.uint8).compute(),
            paths[&#34;prob&#34;],
            overwrite=True,
        )

        _log.info(&#34;collecting the stac json and write out.&#34;)

        processing_dt = datetime.datetime.now()

        uuid_hex = uuid.uuid4()
        remoe_path = dict((k, osp.basename(p)) for k, p in paths.items())
        remote_metadata_path = metadata_path.replace(
            self.config.DATA_PATH, self.config.REMOTE_PATH
        )
        stac_doc = StacIntoDc.render_metadata(
            self.config.product,
            geobox_used,
            (x, y),
            self.config.datetime_range,
            uuid_hex,
            remoe_path,
            remote_metadata_path,
            processing_dt,
        )

        with open(metadata_path, &#34;w&#34;) as fh:
            json.dump(stac_doc, fh, indent=2)


def calculate_indices(ds: xr.Dataset) -&gt; xr.Dataset:
    &#34;&#34;&#34;
    add calculate_indices into the datasets
    @param ds: input ds with nir, red, green bands
    @return: ds with new bands
    &#34;&#34;&#34;
    inices_dict = {
        &#34;NDVI&#34;: lambda ds: (ds.nir - ds.red) / (ds.nir + ds.red),
        &#34;LAI&#34;: lambda ds: (
            3.618
            * ((2.5 * (ds.nir - ds.red)) / (ds.nir + 6 * ds.red - 7.5 * ds.blue + 1))
            - 0.118
        ),
        &#34;MNDWI&#34;: lambda ds: (ds.green - ds.swir_1) / (ds.green + ds.swir_1),
    }

    for k, func in inices_dict.items():
        ds[k] = func(ds)

    ds[&#34;sdev&#34;] = -np.log(ds[&#34;sdev&#34;])
    ds[&#34;bcdev&#34;] = -np.log(ds[&#34;bcdev&#34;])
    ds[&#34;edev&#34;] = -np.log(ds[&#34;edev&#34;])

    return ds


def merge_tifs_into_ds(
    root_fld: str,
    tifs: List[str],
    rename_dict: Optional[Dict] = None,
    tifs_min_num=8,
) -&gt; xr.Dataset:
    &#34;&#34;&#34;
    use os.walk to get the all files under a folder, it just merge the half year tifs.
    We need combine two half-year tifs ds and add (calculated indices, rainfall, and slope)
    @param tifs: tifs with the bands
    @param root_fld: the parent folder for the sub_fld
    @param tifs_min_num: geo-median tifs is 16 a tile idx
    @param rename_dict: we can put the rename dictionary here
    @return:
    &#34;&#34;&#34;
    # TODO: create dummy datasets to test mergue tis
    assert len(tifs) &gt; tifs_min_num
    cache = []
    for tif in tifs:
        if tif.endswith(&#34;.tif&#34;):
            band_name = re.search(r&#34;_([A-Za-z0-9]+).tif&#34;, tif).groups()[0]
            if band_name in [&#34;rgba&#34;, &#34;COUNT&#34;]:
                continue

            band_array = assign_crs(
                xr.open_rasterio(osp.join(root_fld, tif))
                .squeeze()
                .to_dataset(name=band_name),
                crs=&#34;epsg:6933&#34;,
            )
            cache.append(band_array)
    # clean up output
    output = xr.merge(cache).squeeze()
    output.attrs[&#34;crs&#34;] = &#34;epsg:{}&#34;.format(output[&#34;spatial_ref&#34;].values)
    output.attrs[&#34;tile-task-str&#34;] = &#34;/&#34;.join(root_fld.split(&#34;/&#34;)[-3:])
    output = output.drop([&#34;spatial_ref&#34;, &#34;band&#34;])
    return output.rename(rename_dict) if rename_dict else output


def chirp_clip(ds: xr.Dataset, chirps: xr.DataArray) -&gt; xr.DataArray:
    &#34;&#34;&#34;
     fill na with mean on chirps data
    :param ds: geomedian collected with certain geobox
    :param chirps: rainfall data
    :return: chirps data without na
    &#34;&#34;&#34;
    # TODO: test with dummy ds and chirps
    # Clip CHIRPS to ~ S2 tile boundaries so we can handle NaNs local to S2 tile
    xmin, xmax = ds.x.values[0], ds.x.values[-1]
    ymin, ymax = ds.y.values[0], ds.y.values[-1]
    inProj = Proj(&#34;epsg:6933&#34;)
    outProj = Proj(&#34;epsg:4326&#34;)
    xmin, ymin = transform(inProj, outProj, xmin, ymin)
    xmax, ymax = transform(inProj, outProj, xmax, ymax)

    # create lat/lon indexing slices - buffer S2 bbox by 0.05deg
    # Todo: xmin &lt; 0 and xmax &lt; 0,  x_slice = [], unit tests
    if (xmin &lt; 0) &amp; (xmax &lt; 0):
        x_slice = list(np.arange(xmin + 0.05, xmax - 0.05, -0.05))
    else:
        x_slice = list(np.arange(xmax - 0.05, xmin + 0.05, 0.05))

    if (ymin &lt; 0) &amp; (ymax &lt; 0):
        y_slice = list(np.arange(ymin + 0.05, ymax - 0.05, -0.05))
    else:
        y_slice = list(np.arange(ymin - 0.05, ymax + 0.05, 0.05))

    # index global chirps using buffered s2 tile bbox
    chirps = assign_crs(chirps.sel(x=y_slice, y=x_slice, method=&#34;nearest&#34;))

    # fill any NaNs in CHIRPS with local (s2-tile bbox) mean
    return chirps.fillna(chirps.mean())


def complete_gm_mads(era_base_ds: xr.Dataset, geobox: GeoBox, era: str) -&gt; xr.Dataset:
    &#34;&#34;&#34;
    merge the geomedian and rainfall chirps data together
    :param era_base_ds:
    :param geobox:
    :param era:
    :return:
    &#34;&#34;&#34;
    # TODO: this is half year data, require integration tests
    gm_mads = assign_crs(calculate_indices(era_base_ds))

    rainfall = assign_crs(
        xr.open_rasterio(FeaturePathConfig.rainfall_path[era]), crs=&#34;epsg:4326&#34;
    )

    rainfall = chirp_clip(gm_mads, rainfall)

    rainfall = (
        xr_reproject(rainfall, geobox, &#34;bilinear&#34;)
        .drop([&#34;band&#34;, &#34;spatial_ref&#34;])
        .squeeze()
    )
    gm_mads[&#34;rain&#34;] = rainfall

    return gm_mads.rename(
        dict((var_name, str(var_name) + era.upper()) for var_name in gm_mads.data_vars)
    )


def down_scale_gm_band(
    ds: xr.Dataset, exclude: Tuple[str, str] = (&#34;sdev&#34;, &#34;bcdev&#34;), scale=10_000
) -&gt; xr.Dataset:
    for band in ds.data_vars:
        if band not in exclude:
            ds[band] = ds[band] / scale
    return ds


def get_xy_from_task(taskstr: str) -&gt; Tuple[int, int]:
    x_str, y_str = taskstr.split(&#34;/&#34;)[:2]
    return int(x_str.replace(&#34;x&#34;, &#34;&#34;)), int(y_str.replace(&#34;y&#34;, &#34;&#34;))


def extract_dt_from_model_path(path: str) -&gt; str:
    return re.search(r&#34;_(\d{8})&#34;, path).groups()[0]


def extract_xy_from_title(title: str) -&gt; Tuple[int, int]:
    &#34;&#34;&#34;
    split the x, y out from title
    @param title:
    @return:
    &#34;&#34;&#34;
    x_str, y_str = title.split(&#34;,&#34;)
    return int(x_str), int(y_str)


def get_tifs_paths(dirname: str, subfld: str) -&gt; Dict[str, List[str]]:
    &#34;&#34;&#34;
    generated src tifs dictionnary, season on and two, or more seasons

    &#34;&#34;&#34;
    all_tifs = os.walk(osp.join(dirname, subfld))
    # l0_dir, l0_subfld, _ = all_tifs[0]
    return dict(
        (l1_dir, l1_files)
        for level, (l1_dir, _, l1_files) in enumerate(all_tifs)
        if level &gt; 0 and (&#34;.ipynb&#34; not in l1_dir)
    )


@click.command(&#34;tile-predict&#34;)
@click.argument(&#34;task-str&#34;, type=str, nargs=1)
def main(task_str):
    worker = PredictFromFeature()
    worker.run(task_str)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="dea_ml.core.merge_tifs_to_ds.calculate_indices"><code class="name flex">
<span>def <span class="ident">calculate_indices</span></span>(<span>ds: xarray.core.dataset.Dataset) ‑> xarray.core.dataset.Dataset</span>
</code></dt>
<dd>
<div class="desc"><p>add calculate_indices into the datasets
@param ds: input ds with nir, red, green bands
@return: ds with new bands</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def calculate_indices(ds: xr.Dataset) -&gt; xr.Dataset:
    &#34;&#34;&#34;
    add calculate_indices into the datasets
    @param ds: input ds with nir, red, green bands
    @return: ds with new bands
    &#34;&#34;&#34;
    inices_dict = {
        &#34;NDVI&#34;: lambda ds: (ds.nir - ds.red) / (ds.nir + ds.red),
        &#34;LAI&#34;: lambda ds: (
            3.618
            * ((2.5 * (ds.nir - ds.red)) / (ds.nir + 6 * ds.red - 7.5 * ds.blue + 1))
            - 0.118
        ),
        &#34;MNDWI&#34;: lambda ds: (ds.green - ds.swir_1) / (ds.green + ds.swir_1),
    }

    for k, func in inices_dict.items():
        ds[k] = func(ds)

    ds[&#34;sdev&#34;] = -np.log(ds[&#34;sdev&#34;])
    ds[&#34;bcdev&#34;] = -np.log(ds[&#34;bcdev&#34;])
    ds[&#34;edev&#34;] = -np.log(ds[&#34;edev&#34;])

    return ds</code></pre>
</details>
</dd>
<dt id="dea_ml.core.merge_tifs_to_ds.chirp_clip"><code class="name flex">
<span>def <span class="ident">chirp_clip</span></span>(<span>ds: xarray.core.dataset.Dataset, chirps: xarray.core.dataarray.DataArray) ‑> xarray.core.dataarray.DataArray</span>
</code></dt>
<dd>
<div class="desc"><p>fill na with mean on chirps data
:param ds: geomedian collected with certain geobox
:param chirps: rainfall data
:return: chirps data without na</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def chirp_clip(ds: xr.Dataset, chirps: xr.DataArray) -&gt; xr.DataArray:
    &#34;&#34;&#34;
     fill na with mean on chirps data
    :param ds: geomedian collected with certain geobox
    :param chirps: rainfall data
    :return: chirps data without na
    &#34;&#34;&#34;
    # TODO: test with dummy ds and chirps
    # Clip CHIRPS to ~ S2 tile boundaries so we can handle NaNs local to S2 tile
    xmin, xmax = ds.x.values[0], ds.x.values[-1]
    ymin, ymax = ds.y.values[0], ds.y.values[-1]
    inProj = Proj(&#34;epsg:6933&#34;)
    outProj = Proj(&#34;epsg:4326&#34;)
    xmin, ymin = transform(inProj, outProj, xmin, ymin)
    xmax, ymax = transform(inProj, outProj, xmax, ymax)

    # create lat/lon indexing slices - buffer S2 bbox by 0.05deg
    # Todo: xmin &lt; 0 and xmax &lt; 0,  x_slice = [], unit tests
    if (xmin &lt; 0) &amp; (xmax &lt; 0):
        x_slice = list(np.arange(xmin + 0.05, xmax - 0.05, -0.05))
    else:
        x_slice = list(np.arange(xmax - 0.05, xmin + 0.05, 0.05))

    if (ymin &lt; 0) &amp; (ymax &lt; 0):
        y_slice = list(np.arange(ymin + 0.05, ymax - 0.05, -0.05))
    else:
        y_slice = list(np.arange(ymin - 0.05, ymax + 0.05, 0.05))

    # index global chirps using buffered s2 tile bbox
    chirps = assign_crs(chirps.sel(x=y_slice, y=x_slice, method=&#34;nearest&#34;))

    # fill any NaNs in CHIRPS with local (s2-tile bbox) mean
    return chirps.fillna(chirps.mean())</code></pre>
</details>
</dd>
<dt id="dea_ml.core.merge_tifs_to_ds.complete_gm_mads"><code class="name flex">
<span>def <span class="ident">complete_gm_mads</span></span>(<span>era_base_ds: xarray.core.dataset.Dataset, geobox: datacube.utils.geometry._base.GeoBox, era: str) ‑> xarray.core.dataset.Dataset</span>
</code></dt>
<dd>
<div class="desc"><p>merge the geomedian and rainfall chirps data together
:param era_base_ds:
:param geobox:
:param era:
:return:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def complete_gm_mads(era_base_ds: xr.Dataset, geobox: GeoBox, era: str) -&gt; xr.Dataset:
    &#34;&#34;&#34;
    merge the geomedian and rainfall chirps data together
    :param era_base_ds:
    :param geobox:
    :param era:
    :return:
    &#34;&#34;&#34;
    # TODO: this is half year data, require integration tests
    gm_mads = assign_crs(calculate_indices(era_base_ds))

    rainfall = assign_crs(
        xr.open_rasterio(FeaturePathConfig.rainfall_path[era]), crs=&#34;epsg:4326&#34;
    )

    rainfall = chirp_clip(gm_mads, rainfall)

    rainfall = (
        xr_reproject(rainfall, geobox, &#34;bilinear&#34;)
        .drop([&#34;band&#34;, &#34;spatial_ref&#34;])
        .squeeze()
    )
    gm_mads[&#34;rain&#34;] = rainfall

    return gm_mads.rename(
        dict((var_name, str(var_name) + era.upper()) for var_name in gm_mads.data_vars)
    )</code></pre>
</details>
</dd>
<dt id="dea_ml.core.merge_tifs_to_ds.down_scale_gm_band"><code class="name flex">
<span>def <span class="ident">down_scale_gm_band</span></span>(<span>ds: xarray.core.dataset.Dataset, exclude: Tuple[str, str] = ('sdev', 'bcdev'), scale=10000) ‑> xarray.core.dataset.Dataset</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def down_scale_gm_band(
    ds: xr.Dataset, exclude: Tuple[str, str] = (&#34;sdev&#34;, &#34;bcdev&#34;), scale=10_000
) -&gt; xr.Dataset:
    for band in ds.data_vars:
        if band not in exclude:
            ds[band] = ds[band] / scale
    return ds</code></pre>
</details>
</dd>
<dt id="dea_ml.core.merge_tifs_to_ds.extract_dt_from_model_path"><code class="name flex">
<span>def <span class="ident">extract_dt_from_model_path</span></span>(<span>path: str) ‑> str</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def extract_dt_from_model_path(path: str) -&gt; str:
    return re.search(r&#34;_(\d{8})&#34;, path).groups()[0]</code></pre>
</details>
</dd>
<dt id="dea_ml.core.merge_tifs_to_ds.extract_xy_from_title"><code class="name flex">
<span>def <span class="ident">extract_xy_from_title</span></span>(<span>title: str) ‑> Tuple[int, int]</span>
</code></dt>
<dd>
<div class="desc"><p>split the x, y out from title
@param title:
@return:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def extract_xy_from_title(title: str) -&gt; Tuple[int, int]:
    &#34;&#34;&#34;
    split the x, y out from title
    @param title:
    @return:
    &#34;&#34;&#34;
    x_str, y_str = title.split(&#34;,&#34;)
    return int(x_str), int(y_str)</code></pre>
</details>
</dd>
<dt id="dea_ml.core.merge_tifs_to_ds.get_max_cpu"><code class="name flex">
<span>def <span class="ident">get_max_cpu</span></span>(<span>) ‑> int</span>
</code></dt>
<dd>
<div class="desc"><p>Max available CPU (rounded up if fractional), takes into account pod
resource allocation</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_max_cpu() -&gt; int:
    &#34;&#34;&#34;
    Max available CPU (rounded up if fractional), takes into account pod
    resource allocation
    &#34;&#34;&#34;
    ncpu = get_cpu_quota()
    if ncpu is not None:
        return int(math.ceil(ncpu))
    return psutil.cpu_count()</code></pre>
</details>
</dd>
<dt id="dea_ml.core.merge_tifs_to_ds.get_max_mem"><code class="name flex">
<span>def <span class="ident">get_max_mem</span></span>(<span>) ‑> int</span>
</code></dt>
<dd>
<div class="desc"><p>Max available memory, takes into account pod resource allocation</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_max_mem() -&gt; int:
    &#34;&#34;&#34;
    Max available memory, takes into account pod resource allocation
    &#34;&#34;&#34;
    total = psutil.virtual_memory().total
    mem_quota = get_mem_quota()
    if mem_quota is None:
        return total
    return min(mem_quota, total)</code></pre>
</details>
</dd>
<dt id="dea_ml.core.merge_tifs_to_ds.get_tifs_paths"><code class="name flex">
<span>def <span class="ident">get_tifs_paths</span></span>(<span>dirname: str, subfld: str) ‑> Dict[str, List[str]]</span>
</code></dt>
<dd>
<div class="desc"><p>generated src tifs dictionnary, season on and two, or more seasons</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_tifs_paths(dirname: str, subfld: str) -&gt; Dict[str, List[str]]:
    &#34;&#34;&#34;
    generated src tifs dictionnary, season on and two, or more seasons

    &#34;&#34;&#34;
    all_tifs = os.walk(osp.join(dirname, subfld))
    # l0_dir, l0_subfld, _ = all_tifs[0]
    return dict(
        (l1_dir, l1_files)
        for level, (l1_dir, _, l1_files) in enumerate(all_tifs)
        if level &gt; 0 and (&#34;.ipynb&#34; not in l1_dir)
    )</code></pre>
</details>
</dd>
<dt id="dea_ml.core.merge_tifs_to_ds.get_xy_from_task"><code class="name flex">
<span>def <span class="ident">get_xy_from_task</span></span>(<span>taskstr: str) ‑> Tuple[int, int]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_xy_from_task(taskstr: str) -&gt; Tuple[int, int]:
    x_str, y_str = taskstr.split(&#34;/&#34;)[:2]
    return int(x_str.replace(&#34;x&#34;, &#34;&#34;)), int(y_str.replace(&#34;y&#34;, &#34;&#34;))</code></pre>
</details>
</dd>
<dt id="dea_ml.core.merge_tifs_to_ds.merge_tifs_into_ds"><code class="name flex">
<span>def <span class="ident">merge_tifs_into_ds</span></span>(<span>root_fld: str, tifs: List[str], rename_dict: Union[Dict, NoneType] = None, tifs_min_num=8) ‑> xarray.core.dataset.Dataset</span>
</code></dt>
<dd>
<div class="desc"><p>use os.walk to get the all files under a folder, it just merge the half year tifs.
We need combine two half-year tifs ds and add (calculated indices, rainfall, and slope)
@param tifs: tifs with the bands
@param root_fld: the parent folder for the sub_fld
@param tifs_min_num: geo-median tifs is 16 a tile idx
@param rename_dict: we can put the rename dictionary here
@return:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def merge_tifs_into_ds(
    root_fld: str,
    tifs: List[str],
    rename_dict: Optional[Dict] = None,
    tifs_min_num=8,
) -&gt; xr.Dataset:
    &#34;&#34;&#34;
    use os.walk to get the all files under a folder, it just merge the half year tifs.
    We need combine two half-year tifs ds and add (calculated indices, rainfall, and slope)
    @param tifs: tifs with the bands
    @param root_fld: the parent folder for the sub_fld
    @param tifs_min_num: geo-median tifs is 16 a tile idx
    @param rename_dict: we can put the rename dictionary here
    @return:
    &#34;&#34;&#34;
    # TODO: create dummy datasets to test mergue tis
    assert len(tifs) &gt; tifs_min_num
    cache = []
    for tif in tifs:
        if tif.endswith(&#34;.tif&#34;):
            band_name = re.search(r&#34;_([A-Za-z0-9]+).tif&#34;, tif).groups()[0]
            if band_name in [&#34;rgba&#34;, &#34;COUNT&#34;]:
                continue

            band_array = assign_crs(
                xr.open_rasterio(osp.join(root_fld, tif))
                .squeeze()
                .to_dataset(name=band_name),
                crs=&#34;epsg:6933&#34;,
            )
            cache.append(band_array)
    # clean up output
    output = xr.merge(cache).squeeze()
    output.attrs[&#34;crs&#34;] = &#34;epsg:{}&#34;.format(output[&#34;spatial_ref&#34;].values)
    output.attrs[&#34;tile-task-str&#34;] = &#34;/&#34;.join(root_fld.split(&#34;/&#34;)[-3:])
    output = output.drop([&#34;spatial_ref&#34;, &#34;band&#34;])
    return output.rename(rename_dict) if rename_dict else output</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="dea_ml.core.merge_tifs_to_ds.PredictFromFeature"><code class="flex name class">
<span>class <span class="ident">PredictFromFeature</span></span>
<span>(</span><span>config: Union[<a title="dea_ml.core.product_feature_config.FeaturePathConfig" href="product_feature_config.html#dea_ml.core.product_feature_config.FeaturePathConfig">FeaturePathConfig</a>, NoneType] = None, geobox_dict: Union[Dict, NoneType] = None, client: Union[distributed.client.Client, NoneType] = None, gm_ds: Union[xarray.core.dataset.Dataset, NoneType] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>This only covers 2019 case in sandbox now. Check configureation in FeaturePathConfig before use
run this.</p>
<h1 id="todo-add-context-to-this-classpredicted">todo: add context
to this classpredicted</h1></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PredictFromFeature:
    &#34;&#34;&#34;
    This only covers 2019 case in sandbox now. Check configureation in FeaturePathConfig before use
    run this.
    # todo: add context  to this classpredicted
    &#34;&#34;&#34;

    def __init__(
        self,
        config: Optional[FeaturePathConfig] = None,
        geobox_dict: Optional[Dict] = None,
        client: Optional[Client] = None,
        gm_ds: Optional[xr.Dataset] = None,
    ):
        self.config = config if config else FeaturePathConfig()
        self.geobox_dict = geobox_dict
        if not client:
            nthreads = get_max_cpu()
            memory_limit = get_max_mem()
            client = start_local_dask(
                threads_per_worker=nthreads,
                processes=False,
                memory_limit=int(0.9 * memory_limit),
            )
            configure_s3_access(aws_unsigned=True, cloud_defaults=True, client=client)
        self.client = client
        self.gm_ds = gm_ds

    def merge_ds_exec(self, x: int, y: int) -&gt; Tuple[str, xr.Dataset]:
        &#34;&#34;&#34;
        merge the xarray dataset
        @param x: tile index x
        #param y: time inde y
        @return: subfolder path and the xarray dataset of the features
        &#34;&#34;&#34;
        subfld = &#34;x{x:+04d}/y{y:+04d}&#34;.format(x=x, y=y)
        P6M_tifs: Dict = get_tifs_paths(self.config.TIF_path, subfld)
        geobox = self.geobox_dict[(x, y)]
        seasoned_ds = {}
        for k, tifs in P6M_tifs.items():
            era = &#34;_S1&#34; if &#34;2019-01--P6M&#34; in k else &#34;_S2&#34;
            if not self.gm_ds:
                # no prepare base ds
                base_ds = merge_tifs_into_ds(
                    k, tifs, rename_dict=self.config.rename_dict
                )
            else:
                base_ds = self.gm_ds
            # TODO: to validate the 6month geomedia is down scaled already.
            base_ds = down_scale_gm_band(base_ds)

            seasoned_ds[era] = complete_gm_mads(base_ds, geobox, era)

        slope = (
            rio_slurp_xarray(self.config.url_slope, gbox=geobox)
            .drop(&#34;spatial_ref&#34;)
            .to_dataset(name=&#34;slope&#34;)
        )
        return (
            subfld,
            xr.merge(
                [seasoned_ds[&#34;_S1&#34;], seasoned_ds[&#34;_S2&#34;], slope], compat=&#34;override&#34;
            ).chunk({&#34;x&#34;: -1, &#34;y&#34;: -1}),
        )

    def run(self, taskstr: str):
        &#34;&#34;&#34;
        run the prediction here, default crs=&#39;epsg:4326&#39;
        The sample of a feature:
        @return: None
        &#34;&#34;&#34;
        setup_logging()
        import logging  # noqa pylint: disable=import-outside-toplevel

        _log = logging.getLogger(__name__)
        if not self.geobox_dict:
            self.geobox_dict = AfricaGeobox(
                resolution=self.config.resolution,
                crs=self.config.output_crs,
            ).geobox_dict

        x, y = get_xy_from_task(taskstr)

        # step 1: collect the geomedian, indices, rainfall, slope as feature
        # TODO: add more interface in merge_ds_exec, dc.load and x, y
        # adapt to other africa_geobox type
        subfld, data = self.merge_ds_exec(x, y)
        input_data = data[self.config.training_features]

        # step 2: load trained model
        model = joblib.load(self.config.model_path)

        # step 3: prediction
        predicted = predict_xr(
            model,
            input_data,
            clean=True,
            proba=True,
        )

        _log.info(&#34;... Dask computing ...&#34;)
        # predicted.compute()
        predicted.persist()

        # post prediction filtering
        predict = predicted.Predictions
        query = self.config.query.copy()
        # Update dc query with geometry
        geobox_used = self.geobox_dict[(x, y)]
        query[&#34;geopolygon&#34;] = Geometry(geobox_used.extent.geom, crs=geobox_used.crs)

        dc = Datacube(app=__name__)
        # mask with WOFS
        # wofs_query = query.pop(&#34;measurements&#34;)
        wofs = dc.load(product=&#34;ga_ls8c_wofs_2_summary&#34;, **query)
        wofs = wofs.frequency &gt; 0.2  # threshold
        predict = predict.where(~wofs, 0)

        # mask steep slopes
        slope = data.slope &gt; 35
        predict = predict.where(~slope, 0)

        # mask where the elevation is above 3600m
        query.pop(&#34;time&#34;)
        elevation = dc.load(product=&#34;srtm&#34;, **query)
        elevation = elevation.elevation &gt; 3600
        predict = predict.where(~elevation.squeeze(), 0)
        predict = predict.astype(np.uint8)

        output_fld, paths, metadata_path = prepare_the_io_path(self.config, subfld)

        if not osp.exists(output_fld):
            os.makedirs(output_fld)

        _log.info(&#34;collecting mask and write cog.&#34;)
        write_cog(
            predict.compute(),
            paths[&#34;mask&#34;],
            overwrite=True,
        )

        _log.info(&#34;collecting prob and write cog.&#34;)
        write_cog(
            predicted.Probabilities.astype(np.uint8).compute(),
            paths[&#34;prob&#34;],
            overwrite=True,
        )

        _log.info(&#34;collecting the stac json and write out.&#34;)

        processing_dt = datetime.datetime.now()

        uuid_hex = uuid.uuid4()
        remoe_path = dict((k, osp.basename(p)) for k, p in paths.items())
        remote_metadata_path = metadata_path.replace(
            self.config.DATA_PATH, self.config.REMOTE_PATH
        )
        stac_doc = StacIntoDc.render_metadata(
            self.config.product,
            geobox_used,
            (x, y),
            self.config.datetime_range,
            uuid_hex,
            remoe_path,
            remote_metadata_path,
            processing_dt,
        )

        with open(metadata_path, &#34;w&#34;) as fh:
            json.dump(stac_doc, fh, indent=2)</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="dea_ml.core.merge_tifs_to_ds.PredictFromFeature.merge_ds_exec"><code class="name flex">
<span>def <span class="ident">merge_ds_exec</span></span>(<span>self, x: int, y: int) ‑> Tuple[str, xarray.core.dataset.Dataset]</span>
</code></dt>
<dd>
<div class="desc"><p>merge the xarray dataset
@param x: tile index x</p>
<h1 id="param-y-time-inde-y">param y: time inde y</h1>
<p>@return: subfolder path and the xarray dataset of the features</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def merge_ds_exec(self, x: int, y: int) -&gt; Tuple[str, xr.Dataset]:
    &#34;&#34;&#34;
    merge the xarray dataset
    @param x: tile index x
    #param y: time inde y
    @return: subfolder path and the xarray dataset of the features
    &#34;&#34;&#34;
    subfld = &#34;x{x:+04d}/y{y:+04d}&#34;.format(x=x, y=y)
    P6M_tifs: Dict = get_tifs_paths(self.config.TIF_path, subfld)
    geobox = self.geobox_dict[(x, y)]
    seasoned_ds = {}
    for k, tifs in P6M_tifs.items():
        era = &#34;_S1&#34; if &#34;2019-01--P6M&#34; in k else &#34;_S2&#34;
        if not self.gm_ds:
            # no prepare base ds
            base_ds = merge_tifs_into_ds(
                k, tifs, rename_dict=self.config.rename_dict
            )
        else:
            base_ds = self.gm_ds
        # TODO: to validate the 6month geomedia is down scaled already.
        base_ds = down_scale_gm_band(base_ds)

        seasoned_ds[era] = complete_gm_mads(base_ds, geobox, era)

    slope = (
        rio_slurp_xarray(self.config.url_slope, gbox=geobox)
        .drop(&#34;spatial_ref&#34;)
        .to_dataset(name=&#34;slope&#34;)
    )
    return (
        subfld,
        xr.merge(
            [seasoned_ds[&#34;_S1&#34;], seasoned_ds[&#34;_S2&#34;], slope], compat=&#34;override&#34;
        ).chunk({&#34;x&#34;: -1, &#34;y&#34;: -1}),
    )</code></pre>
</details>
</dd>
<dt id="dea_ml.core.merge_tifs_to_ds.PredictFromFeature.run"><code class="name flex">
<span>def <span class="ident">run</span></span>(<span>self, taskstr: str)</span>
</code></dt>
<dd>
<div class="desc"><p>run the prediction here, default crs='epsg:4326'
The sample of a feature:
@return: None</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run(self, taskstr: str):
    &#34;&#34;&#34;
    run the prediction here, default crs=&#39;epsg:4326&#39;
    The sample of a feature:
    @return: None
    &#34;&#34;&#34;
    setup_logging()
    import logging  # noqa pylint: disable=import-outside-toplevel

    _log = logging.getLogger(__name__)
    if not self.geobox_dict:
        self.geobox_dict = AfricaGeobox(
            resolution=self.config.resolution,
            crs=self.config.output_crs,
        ).geobox_dict

    x, y = get_xy_from_task(taskstr)

    # step 1: collect the geomedian, indices, rainfall, slope as feature
    # TODO: add more interface in merge_ds_exec, dc.load and x, y
    # adapt to other africa_geobox type
    subfld, data = self.merge_ds_exec(x, y)
    input_data = data[self.config.training_features]

    # step 2: load trained model
    model = joblib.load(self.config.model_path)

    # step 3: prediction
    predicted = predict_xr(
        model,
        input_data,
        clean=True,
        proba=True,
    )

    _log.info(&#34;... Dask computing ...&#34;)
    # predicted.compute()
    predicted.persist()

    # post prediction filtering
    predict = predicted.Predictions
    query = self.config.query.copy()
    # Update dc query with geometry
    geobox_used = self.geobox_dict[(x, y)]
    query[&#34;geopolygon&#34;] = Geometry(geobox_used.extent.geom, crs=geobox_used.crs)

    dc = Datacube(app=__name__)
    # mask with WOFS
    # wofs_query = query.pop(&#34;measurements&#34;)
    wofs = dc.load(product=&#34;ga_ls8c_wofs_2_summary&#34;, **query)
    wofs = wofs.frequency &gt; 0.2  # threshold
    predict = predict.where(~wofs, 0)

    # mask steep slopes
    slope = data.slope &gt; 35
    predict = predict.where(~slope, 0)

    # mask where the elevation is above 3600m
    query.pop(&#34;time&#34;)
    elevation = dc.load(product=&#34;srtm&#34;, **query)
    elevation = elevation.elevation &gt; 3600
    predict = predict.where(~elevation.squeeze(), 0)
    predict = predict.astype(np.uint8)

    output_fld, paths, metadata_path = prepare_the_io_path(self.config, subfld)

    if not osp.exists(output_fld):
        os.makedirs(output_fld)

    _log.info(&#34;collecting mask and write cog.&#34;)
    write_cog(
        predict.compute(),
        paths[&#34;mask&#34;],
        overwrite=True,
    )

    _log.info(&#34;collecting prob and write cog.&#34;)
    write_cog(
        predicted.Probabilities.astype(np.uint8).compute(),
        paths[&#34;prob&#34;],
        overwrite=True,
    )

    _log.info(&#34;collecting the stac json and write out.&#34;)

    processing_dt = datetime.datetime.now()

    uuid_hex = uuid.uuid4()
    remoe_path = dict((k, osp.basename(p)) for k, p in paths.items())
    remote_metadata_path = metadata_path.replace(
        self.config.DATA_PATH, self.config.REMOTE_PATH
    )
    stac_doc = StacIntoDc.render_metadata(
        self.config.product,
        geobox_used,
        (x, y),
        self.config.datetime_range,
        uuid_hex,
        remoe_path,
        remote_metadata_path,
        processing_dt,
    )

    with open(metadata_path, &#34;w&#34;) as fh:
        json.dump(stac_doc, fh, indent=2)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="dea_ml.core" href="index.html">dea_ml.core</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="dea_ml.core.merge_tifs_to_ds.calculate_indices" href="#dea_ml.core.merge_tifs_to_ds.calculate_indices">calculate_indices</a></code></li>
<li><code><a title="dea_ml.core.merge_tifs_to_ds.chirp_clip" href="#dea_ml.core.merge_tifs_to_ds.chirp_clip">chirp_clip</a></code></li>
<li><code><a title="dea_ml.core.merge_tifs_to_ds.complete_gm_mads" href="#dea_ml.core.merge_tifs_to_ds.complete_gm_mads">complete_gm_mads</a></code></li>
<li><code><a title="dea_ml.core.merge_tifs_to_ds.down_scale_gm_band" href="#dea_ml.core.merge_tifs_to_ds.down_scale_gm_band">down_scale_gm_band</a></code></li>
<li><code><a title="dea_ml.core.merge_tifs_to_ds.extract_dt_from_model_path" href="#dea_ml.core.merge_tifs_to_ds.extract_dt_from_model_path">extract_dt_from_model_path</a></code></li>
<li><code><a title="dea_ml.core.merge_tifs_to_ds.extract_xy_from_title" href="#dea_ml.core.merge_tifs_to_ds.extract_xy_from_title">extract_xy_from_title</a></code></li>
<li><code><a title="dea_ml.core.merge_tifs_to_ds.get_max_cpu" href="#dea_ml.core.merge_tifs_to_ds.get_max_cpu">get_max_cpu</a></code></li>
<li><code><a title="dea_ml.core.merge_tifs_to_ds.get_max_mem" href="#dea_ml.core.merge_tifs_to_ds.get_max_mem">get_max_mem</a></code></li>
<li><code><a title="dea_ml.core.merge_tifs_to_ds.get_tifs_paths" href="#dea_ml.core.merge_tifs_to_ds.get_tifs_paths">get_tifs_paths</a></code></li>
<li><code><a title="dea_ml.core.merge_tifs_to_ds.get_xy_from_task" href="#dea_ml.core.merge_tifs_to_ds.get_xy_from_task">get_xy_from_task</a></code></li>
<li><code><a title="dea_ml.core.merge_tifs_to_ds.merge_tifs_into_ds" href="#dea_ml.core.merge_tifs_to_ds.merge_tifs_into_ds">merge_tifs_into_ds</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="dea_ml.core.merge_tifs_to_ds.PredictFromFeature" href="#dea_ml.core.merge_tifs_to_ds.PredictFromFeature">PredictFromFeature</a></code></h4>
<ul class="">
<li><code><a title="dea_ml.core.merge_tifs_to_ds.PredictFromFeature.merge_ds_exec" href="#dea_ml.core.merge_tifs_to_ds.PredictFromFeature.merge_ds_exec">merge_ds_exec</a></code></li>
<li><code><a title="dea_ml.core.merge_tifs_to_ds.PredictFromFeature.run" href="#dea_ml.core.merge_tifs_to_ds.PredictFromFeature.run">run</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>
